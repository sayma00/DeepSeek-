{
  "Ensemble deep learning and EfficientNet for accurate diagnosis of diabetic retinopathy.pdf": "www.nature.com/scientificreports\nOPEN Ensemble deep learning and\nEfficientNet for accurate diagnosis\nof diabetic retinopathy\nLakshay Arora1,12, Sunil K. Singh1,12, Sudhakar Kumar1,12, Hardik Gupta1,12,\nWadee Alhalabi2,12, Varsha Arya3,7,8,12, Shavi Bansal4,12, Kwok Tai Chui5,12 &\nBrij B. Gupta6,9,10,11,12\nDiabetic Retinopathy (DR) stands as a significant global cause of vision impairment, underscoring\nthe critical importance of early detection in mitigating its impact. Addressing this challenge head-on,\nthis study introduces an innovative deep learning framework tailored for DR diagnosis. The proposed\nframework utilizes the EfficientNetB0 architecture to classify diabetic retinopathy severity levels\nfrom retinal images. By harnessing advanced techniques in computer vision and machine learning,\nthe proposed model aims to deliver precise and dependable DR diagnoses. Continuous testing and\nexperimentation shows to the efficiency of the architecture, showcasing promising outcomes that\ncould help in the transformation of both diagnosing and treatment of DR. This framework takes\nhelp from the EfficientNet Machine Learning algorithms and employing advanced CNN layering\ntechniques. The dataset utilized in this study is titled ’Diagnosis of Diabetic Retinopathy’ and is\nsourced from Kaggle. It consists of 35,108 retinal images, classified into five categories: No Diabetic\nRetinopathy (DR), Mild DR, Moderate DR, Severe DR, and Proliferative DR. Through rigorous testing,\nthe framework yields impressive results, boasting an average accuracy of 86.53% and a loss rate\nof 0.5663. A comparison with alternative approaches underscores the effectiveness of EfficientNet\nin handling classification tasks for diabetic retinopathy, particularly highlighting its high accuracy\nand generalizability across DR severity levels. These findings highlight the framework’s potential\nto significantly advance the field of DR diagnosis, given more advanced datasets and more training\nresources which leads it to be offering clinicians a powerful tool for early intervention and improved\npatient outcomes.\nKeywords Diabetic retinopathy, Deep learning, EfficientNet, CNN, Image dataset, Layering\nDiabetic retinopathy, a consequence of diabetes mellitus, manifests as alterations in the blood vessels within\nthe retina. This condition poses a significant threat to vision integrity and can culminate in blindness if not\nappropriately managed. The underlying pathology stems from prolonged exposure to elevated levels of glucose\nin the bloodstream, which gradually compromises the structural integrity of the retinal vasculature. Over time,\nthis vascular damage manifests as microaneurysms, hemorrhages, and exudates, which impair retinal function\nand compromise visual acuity1,2.\nThe progression of diabetic retinopathy is insidious, often characterized by an asymptomatic early phase.\nHowever, as the condition advances, symptoms such as blurred vision, floaters, and visual field deficits may\nemerge, sig\n---CHUNK_BREAK---\nnaling the need for prompt intervention. The hallmark of diabetic retinopathy lies in its propensity\nto induce macular edema and proliferative changes within the retina, further exacerbating visual impairment3.\n1Department of CSE, Chandigarh College of Engineering and Technology, Panjab University, Chandigarh, India.\n2Department of Computer Science, Immersive Virtual Reality Research Group, King Abdulaziz University, Jeddah,\nSaudi Arabia. 3Department of Electrical and Computer Engineering, Lebanese American University, Beirut 1102,\nLebanon. 4Insights2Techinfo, Jaipur, India. 5Hong Kong Metropolitan University (HKMU), Kowloon, Hong Kong.\n6Present address: Department of Computer Science and Information Engineering, Asia University, Taichung 413,\nTaiwan. 7Center for Interdisciplinary Research, University of Petroleum and Energy Studies (UPES), Dehradun,\nIndia. 8UCRD, Chandigarh University, Chandigarh, India. 9Kyung Hee University, 26 Kyungheedae-ro, Dongdaemun-\ngu, Seoul 02447, Korea. 10Symbiosis Centre for Information Technology (SCIT), Symbiosis International University,\nPune, India. 11University of Economics and Human Science, Warsaw, Poland. 12Lakshay Arora, Sunil K. Singh,\nSudhakar Kumar, Hardik Gupta, Wadee Alhalabi, Varsha Arya, Shavi Bansal, Kwok Tai Chui and Brij B. Gupta\ncontributed equally to this work. email: sksingh@ccet.ac.in; bbgupta@asia.edu.tw\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 1\nwww.nature.com/scientificreports/\nFig. 1. Illustration of diabetic retinopathy lesions5.\nFig. 2. Various grade illustrations6.\nRoutine ophthalmic examination plays a pivotal role in the early detection and management of diabetic\nretinopathy. Screening modalities such as fundus photography and optical coherence tomography enable\nclinicians to identify subtle retinal changes before symptomatic onset. Timely intervention is imperative to\nmitigate disease progression and preserve visual function4.\nThe detection of diabetic retinopathy is often aided by identifying specific retinal lesions. One such lesion,\na microaneurysm, appears as a small, round dot on the retina during the initial phases of the disease. These\ndots have well-defined edges and are typically no more than 125 micrometers in diameter. Although there are\nsix distinct types of microaneurysms, treatment approaches remain consistent across all types. Another lesion\nlinked to diabetic retinopathy is the haemorrhage, which presents as a large, irregularly shaped spot on the\nretina, generally exceeding 125 micrometers in size. Additional lesions, known as hard exudates, result from\nplasma leakage and show up as yellow spots with defined borders on the retina. Soft exudates, caused by swelling\nof nerve fibers, take the form of white, oval patches on the retina, as illustrated in Fig. 1.\nThe International Clinical Diabetic Retinopathy Disease Severity Scale (ICDRDSS), introduced by the\nGlobal Diabetic Retinopathy Project Group, is a pivotal classification s\n---CHUNK_BREAK---\nystem designed for the assessment and\ncategorization of diabetic retinopathy (DR).The ICDRDSS comprises five distinct severity levels, each capturing\nthe progression of the disease and facilitate accurate diagnosis and treatment planning.\nDR progression stages are characterized by different disease severity levels. Figure 2 shows some of the grades\nfor Diabetic Retinopathy The findings associated with each grade are as seen in Table 1.\nEach stage of diabetic retinopathy is characterized by unique characteristics and properties, which can\npose challenges for doctors when making a diagnosis. According to a study by Google, the stages of diabetic\nretinopathy may prove difficult to accurately evaluate manually even by well-trained clinicians. So, the concept\nof automatic detection has garnered attention as a means to enhance accuracy and efficiency in the diagnosis of\nDR.\nThe predominant trend in analyzing various models for artificial intelligence (AI) implementation reveals\na reliance on six key constituent AI models, all of which are deeply rooted in the domain of machine learning.\nThese models are CNN, ANN, Other-NN, Fuzzy ML, SVM, RF (Random Forest). This study integrates the\npowerful EfficientNet Framework with the Layering techinques in CNN to build a model that would help\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 2\nwww.nature.com/scientificreports/\nGrade Description\nGrade 0 No DR - No observable indicators of diabetic retinopathy.\nGrade 1 Mild NPDR - Small, circular microaneurysms appear on the retina.\nGrade 2 Moderate NPDR - Changes beyond microaneurysms are present, though the condition has not progressed to severe NPDR.\nSevere NPDR - Any of the following signs are present, without signs of proliferative DR: over 20 intraretinal hemorrhages\nGrade 3 in all four quadrants, evident venous beading in at least two quadrants, or pronounced intraretinal microvascular\nabnormalities (IrMA) in one or more quadrants.\nGrade 4 Proliferative DR - Characterized by the presence of neovascularization, vitreous or preretinal hemorrhages, or both.\nTable 1. International clinical diabetic retinopathy disease severity scale (ICDRDSS)5.\nincrease the accuracy for detection of DR. By leveraging CNN layering techniques and compound scaling in\nEfficientNet, our framework optimally balances performance and computational resources. Through rigorous\nexperimentation, we demonstrate how this model achieves high accuracy in DR classification while providing\ninterpretable outputs that highlight relevant retinal regions. This advancement aims to enhance the practical\napplication of automated DR diagnosis, bridging the gap between model accuracy and clinical interpretability.\nEfficientNet’s central concept is compound scaling, which systematically adjusts network dimensions-such\nas depth, width, and resolution-to enhance performance. This method ensures optimal resource allocation,\nallowing the model to achieve high accuracy while rema\n---CHUNK_BREAK---\nining resource-efficient across various tasks and\noperational constraints7,8.\nThe various key contributions in this research article are given as follows:\n• The authors conducted an extensive review of existing literature on diabetic retinopathy (DR), focusing on\nits pathology, impact on vision, and the critical importance of early detection to prevent vision impairment.\n• They contributed significantly to the conceptualization and design of an innovative deep learning framework\ntailored to DR diagnosis, integrating the EfficientNet architecture and CNN layering techniques to enhance\ndiagnostic accuracy.\n• The authors collaborated in selecting, preparing, and preprocessing the Kaggle dataset, which comprises\n35,108 retinal images categorized across various severity levels of DR, ensuring comprehensive data coverage\nfor training and validation.\n• The authors worked collectively in writing and editing the introduction, providing a clear and structured\noverview of the problem statement, the clinical relevance of the study, and the potential impact of the pro-\nposed solution on DR diagnosis.Further the remaining paper comprises of the following sections. “Literature\nreview” Section of the research includes the Literature Review and contains all the previous recent work done\nin the field. “Preliminary theory” Section of the research includes the background of the framework and the\ntheoretical information regarding the concepts used. “Proposed framework” Section introduces the proposed\nFramework followed by the experimental setup for the research. “Experimental setup” Section includes the\nresults of the research. “Experimental results” Section discusses the results and describes the limitations in\nthe research. “Discussions and limitations” Section of the research includes the conclusion and future scope\nin the present research.\nLiterature review\nDeep learning techniques have shown promising results in the automated diagnosis of diabetic retinopathy,\na significant cause of blindness globally. Multiple studies have contributed to advancements in this field,\ndemonstrating the effectiveness of deep learning models in accurately detecting diabetic retinopathy from\nretinal fundus photographs.\nThe work of Gulshan et al.9 laid a critical foundation, presenting a robust deep learning algorithm capable\nof accurately identifying diabetic retinopathy in retinal fundus photographs. Building on this, Ting et al.10\ndeveloped a system that incorporated retinal images from multiethnic populations with diabetes, emphasizing the\nimportance of diversity in dataset design for improved model generalization. Abràmoff et al.11 further validated\nthe clinical applicability of autonomous AI-based diagnostic systems through a pivotal trial, showcasing their\npotential in real-world primary care environments and facilitating early intervention. Subsequent research efforts\nfocused on enhancing the practical deployment of these models. Li et al.12 introduced an automated grading\nsys\n---CHUNK_BREAK---\ntem designed to identify vision-threatening diabetic retinopathy cases, aiming to streamline urgent referrals\nbased on disease severity. In a similar vein, Bellemo et al.13 conducted clinical validation in African healthcare\nsettings37, highlighting how deep learning models can offer accessible and accurate diagnostic tools to address\nhealthcare disparities38. Bhaskaranand et al.14 also contributed to this area by developing an automated screening\nand monitoring system to facilitate timely detection and management of diabetic retinopathy progression.\nFurther studies have underscored the importance of generalizability and adaptability in model development.\nSahlsten et al.15 conducted a population-based study, emphasizing the necessity of incorporating diverse\npopulation data in model training to enhance robustness. Meanwhile, Raman et al.16 addressed the challenges\nof training deep learning models on non-mydriatic retinal fundus images from electronic health records39,\nenhancing their applicability to real-world clinical datasets. Takahashi et al.17 extended the scope of deep\nlearning applications to include both diabetic retinopathy and glaucoma diagnosis, showcasing the potential\nfor multifaceted disease detection. Burlina et al.18, although focused on age-related macular degeneration,\ndemonstrated the versatility of deep learning models in analyzing retinal images across various ophthalmic\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 3\nwww.nature.com/scientificreports/\nconditions. Bilal et al.29 developed an AI-based system utilizing U-Net and deep learning to automatically detect\nand classify diabetic retinopathy in fundus images, emphasizing the effectiveness of U-Net architectures in image\nsegmentation tasks critical for disease classification. Building on these efforts, Bilal et al.30 used a combination of\nmixed models for classifying diabetic retinopathy severity, showcasing the adaptability of ensemble approaches\nin handling complex disease grading tasks.\nFurther enhancing the detection framework, Bilal et al.31 incorporated transfer learning techniques with\nU-Net, demonstrating an improved model for diabetic retinopathy detection. Their methodology highlighted\nthe role of pretrained models in efficiently capturing essential retinal features from fundus images. Huang\net al.19 introduced a saliency-guided self-supervised transformer called “Ssit,” which enhances diabetic\nretinopathy grading through focused attention on relevant retinal areas, leveraging saliency maps for improved\nfeature extraction. Validated on large datasets, this model demonstrates significant improvements in grading\naccuracy. Thanikachalam et al.20 built on this approach by proposing an optimized deep CNN for detecting\ndiabetic retinopathy and diabetic macular edema, incorporating adaptive learning techniques to achieve high\nclassification accuracy. Bodapati and Balaji21 further innovated by developing a self-adaptive stacking ensem\n---CHUNK_BREAK---\nble\nmodel that combines multiple neural networks with attention mechanisms, yielding high accuracy across\ndifferent imaging conditions.\nAdvancements in interpretability have also been a focal point in recent studies. Bhati et al.22 presented IDANet,\nan interpretable dual attention network40 that enhances diabetic retinopathy grading by focusing on critical retinal\nregions, maintaining diagnostic accuracy while providing clinical interpretability. Sivapriya et al.23 introduced a\nmodel that classifies diabetic retinopathy by emphasizing microvascular structures, highlighting the importance\nof retinal structure analysis. Ohri and Kumar24 further improved detection through a supervised fine-tuned\nmodel, leveraging transfer learning for efficient DR identification. In exploring optimization techniques, Bilal et\nal.32 applied Grey Wolf Optimization with CNN models, enhancing feature selection processes and improving\nclassification accuracy, which is pivotal for early diabetic retinopathy intervention. Another approach from\nBilal et al.33 combined CNNs with weighted filters and adaptive filtering, underscoring the importance of noise\nreduction and data augmentation for accurate classification, especially in retinal image processing.\nA significant contribution by Bilal et al.34 leveraged a CNN-SVD-enhanced Support Vector Machine for\ndetecting vision-threatening diabetic retinopathy, marking a shift toward hybrid models that integrate machine\nlearning with deep learning for robust detection capabilities. Additionally, Bilal et al.35 introduced EdgeSVDNet,\na 5G-enabled framework for diagnosing diabetic retinopathy in real time, which could greatly enhance\naccessibility and speed in remote medical diagnosis. Bilal et al.36 proposed the NIMEQ-SACNet model using\nself-attention mechanisms, tailored for precision medicine applications in diabetic retinopathy. This advanced\nmodel integrates self-attention with CNNs to improve accuracy and adaptability, demonstrating the potential for\npersonalized diagnostic tools in precision medicine frameworks. Luo et al.25 proposed a CNN model that captures\nboth local and long-range dependencies in retinal images, improving classification across diabetic retinopathy\nstages through enhanced representation of retinal features. Romero-Oraá et al.26 introduced an attention-based\nframework to isolate relevant features for DR grading, demonstrating optimized focus in model design. Zhang\net al.27 explored a semi-supervised contrastive learning method, incorporating saliency maps and unlabeled\ndata for improved robustness. Finally, Wong et al.28 utilized transfer learning41 with parameter optimization,\ndemonstrating the efficacy of feature-weighted ECOC ensembles in enhancing diabetic retinopathy diagnostics.\nThese studies collectively are shown in Tables 2, 3 and 4 highlight the advancements in deep learning\ntechniques for diabetic retinopathy diagnosis, emphasizing the importance of early detection and intervention\nin preventing vi\n---CHUNK_BREAK---\nsion loss. The proposed deep learning framework in this paper builds upon and contributes to\nthis body of research by leveraging EfficientNet as the base model and incorporating advanced CNN layering to\nenhance model effectiveness.\nResearch gap\nWhile the existing body of research in diabetic retinopathy (DR) diagnosis has made considerable progress\nusing deep learning techniques, a significant research gap persists in leveraging advanced architectures to\nmaximize diagnostic accuracy and efficiency. Previous studies have largely focused on utilizing architectures\nsuch as Inception, ResNet, and DenseNets for DR detection, which have shown promising results in terms of\naccuracy. However, there remains an opportunity to explore novel architectures that can deliver higher precision\nand robustness in classifying DR severity levels. This study aims to address this gap by introducing a framework\nbased on the EfficientNetB0 architecture, designed specifically to improve diagnostic accuracy and reliability\nfor DR severity classification. Through extensive testing and validation, the proposed model seeks to bridge\nthe existing gap by providing a dependable solution that achieves high accuracy rates and effectively supports\nclinical decision-making in DR diagnosis.\nPreliminary theory\nEfficientNet\nEfficientNet is a family of convolutional neural network (CNN) models42 appreciated for their efficiency and\neffectiveness in image classification tasks. The architecture of EfficientNet models is characterized by a stem,\nfollowed by multiple blocks, each containing sub-blocks. The overall architecture is composed of five modules,\nwhich are combined to create the EfficientNet models43,44. These modules can be seen in Figure 3.\nStem: The initial portion of the network, the stem, is where the foundation for subsequent layers is set. The\ninput image is processed here to extract basic features.\nBlocks: EfficientNet-B0 is comprised of seven blocks, with each contributing to the hierarchical feature\nextraction process. These blocks play a crucial role in the learning of increasingly abstract features as information\nflows through the network.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 4\nwww.nature.com/scientificreports/\nS.\nno. References Methodologies Data source Advantages Disadvantages and gaps\nLimited to binary classification (DR vs.\n1 Gulshan et al.9 CNN-based deep learning model for Retinal fundus images Demonstrated high sensitivity and no DR); lacks granularity in severity\nbinary DR classification from EyePACS dataset specificity in a clinical setting\nlevels and interpretability\nRetinal images from\nCNNs trained on multiethnic retinal\n2 Ting et al.10 images, with system fine-tuned for DR multiethnic EyePACS, Enhanced generalizability across High computational demand; limited\nUS and Singapore ethnicities and improved accuracy explainability for clinical use\nseverity prediction\ndatasets\nAutonomous AI model with Focuses on binary c\n---CHUNK_BREAK---\nlassification; does\nAbràmoff et EyePACS data from High applicability in clinical settings\n3 al.11 convolutional neural networks for DR primary care settings with autonomous operation not provide insights into DR severity\ndiagnosis grading\n4 Li et al.12 Multi-stage CNN framework targeting Large dataset of color Specialized in detecting severe DR Limited interpretability and\nvision-threatening DR fundus photographs cases, improving triaging generalization due to specialized target\nRetinal fundus images Model scalability is constrained;\n5 Bellemo et al.13 AI-based CNN model tailored for DR from African clinical Validated model efficacy in diverse, limited interpretability for practical\nscreening in low-resource settings resource-limited regions\nsettings diagnostic insights\nLimited interpretability in decision-\nBhaskaranand Deep learning-based automated screening Offers continuous monitoring of DR\n6 et al.14 and monitoring system for DR Retinal fundus images progression, supports early detection making, lacking an advanced\nexplanation-guided method\nHigh robustness due to diverse No focused methodology for\n7 Sahlsten et al.15 Developed and validated DL algorithms Multiethnic population population data, ensuring interpretability, which can limit\nfor DR on a large, diverse population data\ngeneralizability clinician trust\nSupports diagnosis using non-\n8 Raman et al.16 Developed DL models on non-mydriatic Electronic health records mydriatic images, allowing more Limited attention mechanisms,\nimages to detect DR interpretability concerns\naccessible and frequent testing\nSupports multi-disease diagnosis, Lacks targeted grading for different\nTakahashi et Dual-purpose model assessing both DR Retinal fundus\n9 al.17 and glaucoma photographs enhancing model utility in broader DR severity stages, gaps in explaining\nophthalmology results for individual conditions\nConvolutional Neural Networks for High accuracy in AMD grading, shows Focus on AMD limits direct\n10 Burlina et al.18 automated grading of age-related macular Color fundus images potential for adaptation to related applicability to DR, interpretability and\ndegeneration (AMD) conditions such as DR DR severity grading are not addressed\nTable 2. Related works in diabetic retinopathy diagnosis part - 1.\nS.\nno. References Methodologies Data source Advantages Disadvantages and gaps\nPublicly\nSelf-supervised transformer with saliency High complexity due to transformer-based\n11 Huang et al.19 maps for improving model attention and available Enhanced model attention and architecture, may require extensive computation\nfundus image interpretability through saliency maps\ngrading accuracy resources\ndatasets\nFundus\nLimited interpretability, potential overfitting\nThanikachalam Optimized CNN with adaptive learning images Efficient in resource usage and shows\n12 et al.20 and hyperparameter tuning from open improved accuracy with adaptive learning due to CNN’s static feature extraction\ncapabilities\nrepositories\nEnsemble of at\n---CHUNK_BREAK---\ntention-based neural May suffer from longer training times and\nBodapati & Proprietary High robustness and accuracy, effective in\n13 Balaji21 networks with stacking for increased DR dataset handling severity prediction resource requirements due to stacking and\nrobustness ensemble strategies\nDR fundus\nImproved interpretability with attention Complex architecture may be computationally\n14 Bhati et al22 Dual attention network focusing on critical images from on specific retinal regions, higher intensive; dependency on quality of retinal\nretinal regions for interpretability large-scale\naccuracy in grading images\ndatasets\nPublic DR Focuses on fine-grained features, Limited scalability, may struggle with low-\nSivapriya et Deep learning model analyzing\n15 al23 microvascular structures in fundus images fundus image potentially enhancing detection quality images where microvascular structures\ndatasets sensitivity are not clear\nSupervised learning with transfer learning, High accuracy due to supervised fine- Dependency on large, labeled datasets; limited\nOhri &\n16 Kumar24 fine-tuning on CNN models for improved Not Specified tuning and transfer learning; improved interpretability due to black-box nature of CNN;\nDR detection accuracy model performance on labeled DR data lacks attention mechanisms for feature focus\nDeep CNN with local and global retinal Retinal Effective in capturing both fine-grained High computational cost due to deep CNN\n17 Luo et al.25 features using long-range dependency Fundus and global retinal features for accurate layers; may require extensive preprocessing of\nmodeling Images DR stage classification images for better feature extraction\nAttention mechanisms isolate relevant Improved interpretability by visualizing Limited to fundus images; potential over-\nRomero-Oraá Fundus Image\n18 et al.26 features, focusing model on key retinal Dataset feature importance; focuses on critical reliance on salient features, may overlook subtle\nregions for grading areas, enhancing decision accuracy signs; lacks robustness across diverse datasets\nSemi-supervised learning with contrastive Reduces dependency on large labeled Complexity in implementing contrastive\n19 Zhang et al.27 approach, uses saliency maps for robust Not Specified datasets; robust against noisy data due to learning; risk of lower performance without\ngrading contrastive learning accurate saliency maps for guidance\nHigh accuracy due to parameter Complexity in hyperparameter tuning;\nTransfer learning optimized with feature-\n20 Wong et al.28 weighted Error-Correcting Output Codes Diverse DR optimization and ensemble approach; potentially high resource requirements due\nImage Dataset feature-weighted ECOC enhances to ensemble approach and ECOC processing\n(ECOC) for ensemble grading\nclassification complexity\nTable 3. Related works in diabetic retinopathy diagnosis part - 2.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 5\nwww.nature.com/scientificreports/\nS.\n\n---CHUNK_BREAK---\nno. References Methodologies Data source Advantages Disadvantages and gaps\nLimited to dataset characteristics,\n21 Bilal et al.29 AI-based automatic detection and classification Retinal fundus images Highly effective in image segmentation lacking generalizability across diverse\nusing U-Net and deep learning for disease classification\npopulations\n22 Bilal et al.30 Mixed models for disease grading and severity Diabetic Retinopathy Improved adaptability of ensemble Mixed models may increase\nclassification Grading Database models for complex grading computational complexity\n23 Bilal et al.31 Transfer learning with U-Net for enhanced Retinal fundus images Efficient feature extraction with Transfer learning limited by domain\ndetection accuracy pretrained models specificity\n24 Bilal et al.32 Grey Wolf Optimization with CNN for feature Retinal images Enhanced feature selection and Optimization technique may not\nselection classification accuracy generalize well to all data types\n25 Bilal et al.33 CNNs with weighted filters and adaptive Retinal fundus images Effective noise reduction and improved Increased model complexity and\nfiltering for classification classification accuracy training time\n26 Bilal et al.34 CNN-SVD-enhanced SVM for detecting Retinal fundus images Robust detection capabilities through Complex hybrid structure may require\nvision-threatening retinopathy hybrid model significant computational resources\n27 Bilal et al.35 EdgeSVDNet, 5G-enabled for real-time Retinal fundus images Enhanced accessibility and speed for Dependent on 5G infrastructure, limited\ndiagnosis with 5G connectivity remote diagnostics in areas without high-speed connectivity\nComplexity of self-attention mechanism\n28 Bilal et al.36 NIMEQ-SACNet model with self-attention for Retinal image data for High accuracy and adaptability for may increase model size and training\nprecision medicine precision diagnostics precision medicine applications\nrequirements\nTable 4. Related works in diabetic retinopathy diagnosis part - 3.\nFig. 3. Types of modules45.\nSub-blocks: Within each block, there are varying numbers of sub-blocks. Specific operations are performed\nby each sub-block to transform the input features.\nModules:\n• Module 1: Serving as the starting point for the first sub-block in the first block.\n• Module 2: Acting as the starting point for the first sub-block in all blocks except the first one.\n• Module 3: Skip connections are established to all sub-blocks, facilitating information flow and aiding in gra-\ndient propagation.\n• Module 4: Utilized for combining skip connections in the first sub-blocks, thereby enhancing feature rep-\nresentation.\n• Module 5: Connecting each sub-block to its preceding sub-block via skip connections and combining them to\nrefine feature maps.Sub-block Types:\n•\n• Sub-block 1: Exclusive to the first sub-block in the first block, initializing the feature extraction process.\n• Sub-block 2: Employed as the first sub-block in subsequent blocks, contributing \n---CHUNK_BREAK---\nto feature refinement.\n• Sub-block 3: Utilized for all sub-blocks except the first one in each block, further enhancing feature rep-\nresentation.By combining these modules and sub-blocks in a specific manner, a balance between model size,\ncomputational cost, and performance is achieved by EfficientNet-B0. It is noteworthy that EfficientNet-B0\ncomprises 237 layers. Thus, EfficientNet models excel in various image classification tasks while remaining\ncomputationally efficient. The basic architecture for the EfficientNet Model can be in Fig. 4.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 6\nwww.nature.com/scientificreports/\nHyperparameters\nHyperparameters are pivotal components in machine learning and deep learning algorithms, as they govern\nthe learning process and influence the resultant model parameters. Distinguished by the prefix ’hyper_’,\nthese parameters serve as high-level controls that shape the learning trajectory and ultimately determine the\ncharacteristics of the trained model.\nUnlike model parameters, which are learned during the training process and directly influence the model’s\npredictions, hyperparameters remain external to the resulting model. They are integral to the learning algorithm’s\nfunctionality, yet they do not become ingrained within the model structure.\nSome notable examples of hyperparameters encompass a diverse array of settings that profoundly impact the\nlearning process and model performance:\n• Train-Test Split Ratio: Dictates the proportion of data allocated for training versus testing, influencing the\nmodel’s ability to generalize to unseen data.\n• Learning Rate: A crucial parameter in optimization algorithms like gradient descent, controlling the magni-\ntude of parameter updates during training.\n• Choice of Optimization Algorithm: Determines the approach used to minimize the model’s loss function, with\noptions including gradient descent, stochastic gradient descent, or advanced techniques like the Adam opti-\nmizer.\n• Choice of Activation Function: Pertains to the nonlinear transformation applied within neural network lay-\ners, with popular functions including Sigmoid, ReLU, and Tanh, impacting the network’s capacity to capture\ncomplex patterns.\n• Cost or Loss Function: Defines the objective function optimized during training, guiding the model towards\nminimizing prediction errors or maximizing performance metrics.\n• Number of Hidden Layers and Activation Units: Crucial architectural decisions in neural networks, influenc-\ning the network’s depth, breadth, and expressive capacity.\n• Dropout Rate: Specifies the probability of randomly dropping neurons during training, a regularization tech-\nnique aimed at preventing overfitting.\n• Number of Training Iterations (Epochs): Specifies how many times the model will pass through the full training\ndataset, influencing both convergence rate and model stability.\n• Number of Clusters: Applicable in clustering tasks, this parameter impacts th\n---CHUNK_BREAK---\ne detail and organization of the\nclusters formed.\n• Kernel or Filter Size in Convolutional Layers: Determines the scope of convolution operations, essential for\ncapturing features from input data within convolutional neural networks.\n• Pooling Size: Sets the dimensions of pooling areas in CNNs, affecting the downsampling of features and the\ndevelopment of spatial hierarchies.\n• Batch Size: Defines the quantity of samples processed per training iteration, impacting both computational\nefficiency and the accuracy of gradient estimation.By judiciously tuning these hyperparameters, practitioners\ncan optimize the learning process, enhance model performance, and achieve superior results across a diverse\nrange of machine learning and deep learning tasks. Table 5 gives a summary for Hyperparameters for different\nMachine Learning Algorithms\nFlatten\nThe Flatten method within the layers module is considered a crucial tool for reshaping data, particularly when\ntransitioning from convolutional layers to fully connected layers in a neural network. This method is essentially\nFig. 4. Architecture for EfficientNet-B045.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 7\nwww.nature.com/scientificreports/\nML algorithm Main HPs Optional HPs HPO methods Libraries\nRidge & lasso Alpha – BO-GP Skopt\nLogistic regression Penalty, c, solver – BO-TPE, SMAC Hyperopt, SMAC\nSkopt, Hyperopt,\nKNN n_neighbors Weights, p, algorithm BOs, Hyperband\nSMAC, Hyperband\nGamma, coef0, BO-TPE, SMAC, Hyperopt, SMAC,\nSVM C, kernel, epsilon (for SVR)\ndegree BOHB BOHB\nNB Alpha – BO-GP Skopt\nCriterion, max_depth, min_samples_split, min_samples_leaf, max_features, splitter, GA, PSO, BO-TPE, TPOT, Optunity,\nDT –\nmin_weight_fraction_leaf, max_leaf_nodes SMAC, BOHB SMAC, BOHB\nn_estimators, max_depth, criterion, min_samples_split, min_samples_leaf, max_ GA, PSO, BO-TPE, TPOT, Optunity,\nRF & ET –\nfeatures, splitter, min_weight_fraction_leaf, max_leaf_nodes SMAC, BOHB SMAC, BOHB\nn_estimators, max_depth, learning_rate, subsample, colsample_bytree, min_child_ GA, PSO, BO-TPE, TPOT, Optunity,\nXGBoost –\nweight, gamma, alpha, lambda SMAC, BOHB SMAC, BOHB\nVoting Estimators, voting weights – GS Sklearn\nmax_samples, Sklearn, Skopt,\nBagging Base_estimator, n_estimators GS, BOs\nmax_features Hyperopt, SMAC\nAdaBoost Base_estimator, n_estimators, learning_rate – BO-TPE, SMAC Hyperopt, SMAC\nNumber of hidden layers, ‘units’ per layer, loss, optimizer, Activation, learning_rate,\nDeep learning dropout rate, epochs, batch_size, early stop patience, number of frozen layers (if – PSO, BOHB Optunity, BOHB\ntransfer learning is used)\nHierarchical Skopt, Hyperopt,\nn_clusters, distance_threshold Linkage BOs, Hyperband\nclustering SMAC, Hyperband\nBO-TPE, SMAC, Hyperopt, SMAC,\nDBSCAN eps, min_samples –\nBOHB BOHB\ncovariance_type,\nGaussian mixture n_components BO-GP Skopt\nmax_iter, tol\nSkopt, Hyperopt,\nPCA n_components svd_solver BOs, Hyperband\nSMAC, Hyperband\nSkopt, Hyperopt,\nLDA n_components solver\n---CHUNK_BREAK---\n, shrinkage BOs, Hyperband\nSMAC, Hyperband\nTable 5. Summary of hyperparameters and hyperparameter optimization (HPO) methods for various machine\nlearning algorithms.\nutilized to collapse or flatten the input tensor into a one-dimensional tensor, which is deemed essential for it to\nbe fed into a dense layer.\nWhen dealing with convolutional layers, the output typically comprises a multi-dimensional tensor, where\neach dimension represents different features extracted by the convolutional filters. However, fully connected\nlayers require a one-dimensional input, where each element corresponds to a single feature. This is where the\nflatten method proves to be handy.\nBelow are few of the most important features for flatten:\n• Input Tensor: A multi-dimensional tensor, typically the output of a convolutional layer, serves as the starting\npoint.\n• Flattening Operation: The flatten method reshapes the tensor by concatenating all the elements along all di-\nmensions except the batch dimension.\n• Output Tensor: The result is a one-dimensional tensor that can be directly fed into a dense layer.Figure 5 rep-\nresents and displays a standard Flatten Layer with inter-layer connections.\nDense\nA dense layer, also referred to as a fully connected layer, is a fundamental component in neural network\narchitectures, especially in feedforward networks. In this layer, each neuron is linked to every neuron in the\nsubsequent layer, enabling comprehensive connectivity between layers.\nIn TensorFlow, adding a dense layer to a neural network model requires specifying the number of neurons or\nunits in that layer, which defines the output space’s dimensionality. Each neuron in a dense layer receives input\nfrom all neurons in the preceding layer, and it computes an output by applying a weighted sum to these inputs,\nfollowed by an activation function. Figure 6 illustrates the typical structure of a dense layer.\nThe parameters of a dense layer include weights and biases. The strength of the connections between neurons\nis represented by the weights, and offsets are learned by the model through biases to better fit the data.\nDense layers are versatile and can be used in various neural network architectures for tasks such as\nclassification, regression, and even unsupervised learning. They’re often stacked together with activation\nfunctions like ReLU (Rectified Linear Unit) or sigmoid to introduce non-linearity into the model, enabling it to\nlearn complex patterns and relationships in the data.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 8\nwww.nature.com/scientificreports/\nFig. 5. Standard flatten layer.\nFig. 6. Standard dense layer.\nProposed framework\nThe proposed framework leverages the EfficientNetB0 model to categorize the severity levels of diabetic\nretinopathy from retinal images. EfficientNetB0 was chosen due to its exceptional accuracy in image classification\ntasks and its computational efficiency, achieved via a compound scaling technique tha\n---CHUNK_BREAK---\nt optimally adjusts the\nmodel’s depth, width, and resolution. In this framework, the upper layers of a pre-trained EfficientNetB0\nmodel (pre-trained on ImageNet) were removed by setting include_top=False, and a new dense layer with five\nneurons was incorporated to classify images into five distinct diabetic retinopathy severity categories. A softmax\nactivation function is applied to the final layer to produce probability distributions across these classes8.\nThe methodology employs an algorithmic sequence: preprocessing starts by scaling pixel values to the [0,1]\nrange and implementing data augmentation strategies like random horizontal flips, rotations, and zooms.\nThe EfficientNetB0 model, initialized with ImageNet weights, is then enhanced by adding a flatten layer and a\ndense layer with softmax activation for classification purposes. Training is fine-tuned with a custom learning\nrate scheduler and early stopping triggered by validation loss, leading to a model that is both accurate and\ncomputationally efficient. This holistic framework delivers optimal performance in diabetic retinopathy classi-\nfication.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 9\nwww.nature.com/scientificreports/\nAlgorithm 1. EfficientNet-Based Model for Diabetic Retinopathy Detection\nModel architecture\nThe EfficientNetB0 model was chosen for this study due to its proven high performance and computational\nefficiency in image classification tasks. It employs a compound scaling strategy that balances the network’s\ndepth, width, and resolution, ensuring both accuracy and efficiency. In this setup, the top layers of the pre-\ntrained EfficientNetB0 (trained on ImageNet) were removed (include_top=False), and a new dense layer with\nfive neurons was added, each representing a diabetic retinopathy severity level. The softmax activation function\nin this final layer generates probability distributions across these classes. The model’s total parameter count was\n4,049,571, of which 4,007,548 were trainable, enabling efficient learning from the retinal images while keeping\ncomputational demands low.\nThe training process used the Adam optimizer for its adaptability in learning rates. Additionally, a custom\nlearning rate scheduler was applied, reducing the rate by half every three epochs to mitigate overfitting. Sparse\nFig. 7. Proposed algorithm flow for EfficientNetB0.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 10\nwww.nature.com/scientificreports/\nCategorical Cross-Entropy was employed as the loss function, suitable for multi-class classification tasks. The\nmodel was trained over 40 epochs with an initial learning rate of 0.001 and a batch size of 32, utilizing data\naugmentation techniques like random flips, rotations, and zooms. Figure 7 illustrates the model’s flow and\nprovides a basic architectural overview of the proposed solution.\nExperimental setup\nThe experimental setup was designed with a focus on improvi\n---CHUNK_BREAK---\nng model generalization through data augmentation\nand careful tuning of hyperparameters. Data augmentation was applied using Keras’ ImageDataGenerator, which\nincluded horizontal flipping, random rotations of 0.1 radians, and random zooming. The model was trained with\na batch size of 32, using images resized to 224 224 pixels. A total of 40 epochs were completed, yielding an\n×\naccuracy of 0.8653 and a loss of 0.5663, as observed in the training logs.\nDataset description\nThe dataset utilized in this study is sourced from Kaggle46. It consists of 35,108 retinal images, classified into\nfive categories: No Diabetic Retinopathy (DR), Mild DR, Moderate DR, Severe DR, and Proliferative DR.\nThe distribution of images is highly imbalanced, with 25,802 images labeled as ’No DR’, 5288 as ’Mild DR’,\n2438 as ’Moderate DR’, 872 as ’Proliferative DR’, and 708 as ’Severe DR’. The images were captured at different\nresolutions, but for the purposes of this study, they were resized to 224 224 pixels to ensure uniform input\n×\ndimensions across the dataset. Preprocessing included the normalization of pixel values to a [0,1] range and data\naugmentation techniques such as random horizontal flipping, zooming, and rotation to introduce variability and\nenhance model generalization.\nData preprocessing\nThe dataset presented a notable imbalance in class distribution, with the dominant class (No DR)\nrepresenting over 70% of the total images. To rectify this disparity, undersampling was employed through the\nRandomUnderSampler function from the imbalanced-learn library. This adjustment reduced the\ncount of ’No DR’ images to match the sample size of the minority class (Severe DR), achieving a balanced dataset\nwith 3704 images. This balancing allowed the model to learn equally from each class, minimizing the likelihood\nof bias toward the majority class during training. The balanced dataset was then divided into 2963 images for\ntraining and 741 images for validation, preserving the balanced class distribution in both subsets.\nAs part of essential preprocessing, data normalization was applied to standardize pixel values across input\nimages. Using TensorFlow’s ImageDataGenerator, pixel values were scaled to a range of [0,1] by dividing\neach pixel by 255. To further enhance the training data, data augmentation techniques were incorporated,\nincluding random horizontal flips, rotations up to 0.1 radians, and zooms up to 10%. These augmentations\nintroduced minor variations to the training images, aiding in reducing overfitting and enhancing the model’s\ngeneralization to new data.\nDeep CNN model for comparison\nIn addition to the EfficientNetB0 model, the deep CNN model from Luo et al.25 was referenced for comparative\nperformance assessment. Luo’s model25 architecture leverages a multi-scale feature fusion approach specifically\ndesigned to improve diabetic retinopathy detection by capturing both local and global retinal features, which\nis particularly beneficial for identifying subtle path\n---CHUNK_BREAK---\nologies. This model comprises multiple convolutional layers,\neach followed by pooling operations, and integrates attention mechanisms to focus on essential image areas for\nenhanced diagnostic accuracy. Additionally, Luo et al.25optimized the model by applying multi-scale feature\nfusion layers, allowing effective feature extraction at different spatial resolutions.\nWhile our EfficientNetB0 model uses data augmentation techniques similar to those described in Luo’s\nstudy25, including random rotations, flips, and zooms, it was trained with a larger batch size of 32 over 40\nepochs. The model was optimized using the Adam optimizer and Sparse Categorical Cross-Entropy as the loss\nfunction. By comparing our EfficientNetB0 model’s performance to Luo et al.25 CNN model, we aim to evaluate\nboth accuracy and computational efficiency in diabetic retinopathy detection. This comparison highlights\nEfficientNetB0’s balance of computational efficiency and robustness, even as Luo’s model25 demonstrates strong\nfeature extraction capabilities through multi-scale processing, enhancing its performance in identifying nuanced\ndiabetic retinopathy features.\nHyperparameters used\nTable 6 below gives a summary for Hyperparameters and their optimization methods for numerous Machine\nLearning algorithms47.\nS. no. Category Model used Parameter Input value\n1 Sequential EfficientNet B0 input_shape (224,224,3)\n2 Sequential EfficientNet B0 weights imagenet\n3 Sequential EfficientNet B0 include_top False\n4 Layers Dense Number of neurons 5\n5 Layers Dense activation softmax\nTable 6. Hyperparameters used in model training.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 11\nwww.nature.com/scientificreports/\nModel evaluation\nThe model’s performance is rigorously assessed to determine its diagnostic precision. Evaluation is conducted\nusing the testing dataset, which was set aside after data partitioning.\nPrecision (P c) is defined as:\nP c =\nTruePositivesc\n(1)\nTruePositivesc+FalsePositivesc\nRecall (R c) is defined as:\nR c =\nTruePositivesc\n(2)\nTruePositivesc+FalseNegatives\nc\nThe F1 Score calculation for a specific class (c) is defined as:\nF1c = 2\nP\n·\nc\nP\n+\nc ·\nR\nR\nc\nc (3)\nExperimental results\nThe EfficientNetB0 model achieved a maximum accuracy of 97.11% with a corresponding loss of 0.1596 after\ntraining for 40 epochs, incorporating data augmentation techniques. Data augmentation, including random\nflips, rotations, and zooms, provided a slight improvement in validation performance, as reflected by an average\naccuracy of 86.53 and an average loss of 0.5663, with 95% confidence intervals for accuracy and loss at (0.8677,\n0.8677) and (0.5529, 0.5529), respectively. During training, accuracy consistently improved, reaching a final\nmaximum training accuracy of 96.87% without data augmentation and 97.11% with augmentation as seen in\nFig. 8. This increase suggests that data augmentation contributes minor but meaningful gains in generalization.\nDespite some variabil\n---CHUNK_BREAK---\nity in validation accuracy, EfficientNetB0 demonstrated competitive performance in\ndiabetic retinopathy classification, indicating its effectiveness in handling this classification task. Table 7\nsummarizes the results as well as shows the comparison with a previous research.\nAfter 40 epochs, the EfficientNetB0 model achieved an average training accuracy of 86.53% with data\naugmentation applied. In comparison, Luo et al.25 multi-scale feature fusion method, although computationally\nintensive, demonstrated robustness in diabetic retinopathy classification, achieving 83.6% accuracy. Luo’s model25\neffectively captures local and global retinal features, a valuable capability for nuanced feature identification.\nHowever, EfficientNetB0 maintains a high accuracy while offering significantly lower computational demand\n(processing time of 1190 seconds versus 4778 seconds for Luo’s model)25, making it highly suitable for resource-\nlimited environments.\nBoth the EfficientNetB0 and Luo et al.25 multi-scale feature fusion model were evaluated on diabetic\nretinopathy classification tasks, with each presenting distinct strengths across key performance metrics. The\nEfficientNetB0 model, leveraging pre-trained weights and optimized architecture, achieved an impressive\nmaximum accuracy of 97.11% and demonstrated higher computational efficiency. Luo’s model25, while excelling\nin feature extraction at multiple spatial scales with a precision of 81.9%, incurs a higher processing cost. This\ncomparison highlights that while both models effectively handle retinal images, EfficientNetB0 offers an optimal\nbalance between computational efficiency and accuracy.\nDiscussions and limitations\nOne significant limitation of this study is the potential bias introduced by the dataset. The ’Diagnosis of Diabetic\nRetinopathy’ dataset, though substantial, may lack diversity in patient demographics such as age, gender, and\nethnic background, potentially affecting the model’s generalizability to other populations. This lack of diversity\ncould result in a model that performs well on certain demographic groups but may yield lower accuracy when\napplied to more diverse or distinct populations, limiting its broader clinical applicability.\nAdditionally, while undersampling was used to address class imbalance, this technique may not fully eliminate\nbias, as it reduces the sample size of the dominant classes rather than enriching minority class representation.\nThis could lead to skewed predictions, particularly in underrepresented classes, possibly resulting in lower\ndiagnostic reliability for certain stages or categories of diabetic retinopathy. Future research should consider\nincorporating datasets that represent a wider range of patient characteristics to enhance model robustness across\ndifferent demographics, potentially mitigating these biases.\nFurthermore, while this study used a balanced dataset to assess model performance, its applicability to real-\nworld, naturally imbalanced data remai\n---CHUNK_BREAK---\nns unexplored. In a real clinical setting, diabetic retinopathy severity\nlevels are unevenly distributed, with milder stages being far more common. A model’s performance on such\nimbalanced data could vary significantly, as it may struggle to detect rare but clinically important cases. Thus,\nexploring model performance on unbalanced datasets is crucial, as it could reveal further tuning needs to ensure\nreliable predictions across all severity levels in practical applications.\nConclusion and future scope\nThis paper proposed a deep learning framework for diagnosing diabetic retinopathy using retinal images.\nThe proposed approach combines the efficiency of EfficientNet with the interpretability of layrering methods\nto achieve accurate and transparent diagnosis. The experimental results demonstrate that EfficientNetB0,\nwith its compound scaling and pre-trained weights, significantly achieves high accuracy and robustness in\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 12\nwww.nature.com/scientificreports/\nFig. 8. Results achieved: (a) Using EfficientNetB0 without data augmentation; (b) Using EfficientNetB0 with\ndata augmentation.\nDR classification. The model’s average performance metrics-accuracy of 0.8653 and loss of 0.5663 with data\naugmentation-further support its efficacy, while EfficientNetB0’s 97.11% maximum accuracy, combined with\nlower computational demands, indicates its practical viability for real-world clinical applications, especially\nwhen computational resources are limited. In contrast, Luo et al.25 approach, while effective in feature extraction,\nmay be more suitable for scenarios where detailed feature mapping and high sensitivity are essential, albeit at a\nhigher computational cost.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 13\nwww.nature.com/scientificreports/\nModel Accuracy Loss\nWithout data augmentation 0.8622 0.5783\nEfficientNetB0\nWith data augmentation 0.8653 0.5663\nDeep CNN Model – 0.836 –\nTable 7. Average accuracy with corresponding loss.\nThere are several avenues for future work that can build upon the findings of this study. An immediate\narea for improvement involves expanding the dataset by integrating clinical data, such as patient history, blood\nglucose levels, and the presence of comorbidities like hypertension and cardiovascular diseases. Such additional\ninformation could provide a more comprehensive view of each patient’s health profile48, enabling more precise\npredictions of diabetic retinopathy progression. Moreover, while this study focused on retinal images, future\nresearch could explore multimodal approaches that combine image data with structured medical data to improve\nboth diagnostic accuracy and model robustness.\nIn terms of model development, investigating more advanced architectures such as Vision Transformers\n(ViTs) or hybrid models incorporating attention mechanisms could further boost diagnostic accuracy. Vision\nTransformers have sh\n---CHUNK_BREAK---\nown great promise in other computer vision tasks and could be adapted for retinal image\nanalysis. Another key direction would be optimizing the model for real-time deployment in clinical settings,\nfocusing on reducing inference time and computational requirements without compromising accuracy. Finally,\nlongitudinal studies that monitor patients over time could yield insights into the model’s ability to predict disease\nprogression, rather than simply classifying the current stage of diabetic retinopathy. Such advancements could\nsignificantly enhance the clinical utility of diabetic retinopathy classification models.\nData availibility\nThe datasets analyzed during the current study are publicly available in the Kaggle repository under the title\n“Diabetic Retinopathy Resized Dataset”46 at h tt p s : / / w w w . k a g g l e . c o m / d a t a s e ts / t a n l i k e sm a t h / d i a b et i c - r e t i n op a t\nh y - r e s iz e d / d a t a .\nReceived: 29 September 2024; Accepted: 25 November 2024\nReferences\n1. Stitt, A. W. et al. The progress in understanding and treatment of diabetic retinopathy. Prog. Retin. Eye Res. 51, 156–186. h t t p s : / / d\no i . o r g / 1 0 .1 0 1 6 / j . p r et e y e r e s . 2 01 5 . 0 8 . 0 0 1 (2016).\n2. Gadekallu, T. et al. Deep neural networks to predict diabetic retinopathy. J. Ambient Intell. Hum. Comput. 14, 5407–5420. h t t p s :/ /\nd o i . o r g /1 0 . 1 0 0 7 / s 12 6 5 2 - 0 2 0 - 01 9 6 3 - 7 (2023).\n3. Silva, P. S. et al. Automated machine learning for predicting diabetic retinopathy progression from ultra-widefield retinal images.\nJAMA Ophthalmol. 142, 171–178. https://doi.org/10.1001/jamaophthalmol.2023.6318 (2024).\n4. Grzybowski, A. et al. Artificial intelligence for diabetic retinopathy screening using color retinal photographs: From development\nto deployment. Ophthalmol. Ther. 12, 1419–1437. https://doi.org/10.1007/s40123-023-00691-3 (2023).\n5. Vijayan, M. A regression-based approach to diabetic retinopathy diagnosis using efficientnet. Diagnostics 13, 774. h t t p s : // d o i . o r g /\n1 0 . 3 3 9 0 / d i ag n o s t i c s 1 30 4 0 7 7 4 (2023).\n6. Ariza López, L. & Ramos, S. Diabetic retinopathy diagnosis using deep learning (2023).\n7. Pramudhita, D. A., Azzahra, F., Arfat, I. K., Magdalena, R. & Saidah, S. Strawberry plant diseases classification using CNN based\non MobileNetV3-large and efficientnet-B0 architecture. Jurnal Ilmiah Teknik Elektro Komputer dan Informatika JITEKI 9, 522–534.\nhttps://doi.org/10.26555/jiteki.v9i3.26341 (2023).\n8. Alhijaj, J. A. & Khudeyer, R. S. Integration of efficientnetb0 and machine learning for fingerprint classification.\nInformatica[SPACE]https://doi.org/10.31449/inf.v47i5.4724 (2023).\n9. Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus\nphotographs. JAMA 316, 2402–2410 (2016).\n10. Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related e\n---CHUNK_BREAK---\nye diseases using\nretinal images from multiethnic populations with diabetes. JAMA 318, 2211–2223 (2017).\n11. Abràmoff, M. D., Lavin, P. T., Birch, M., Shah, N. & Folk, J. C. Pivotal trial of an autonomous ai-based diagnostic system for\ndetection of diabetic retinopathy in primary care offices. NPJ Digital Med. 1, 1–8 (2018).\n12. Li, Z. et al. An automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color\nfundus photographs. Diabetes Care 42, 1584–1589 (2019).\n13. Bellemo, V. et al. Artificial intelligence using deep learning to screen for referable and vision-threatening diabetic retinopathy in\nAfrica: A clinical validation study. Lancet Digit. Health 1, e35–e44 (2019).\n14. Bhaskaranand, M. et al. Automated diabetic retinopathy screening and monitoring using retinal fundus image analysis. J. Diabetes\nSci. Technol. 13, 438–446 (2019).\n15. Sahlsten, J. et al. Development and validation of a deep learning algorithm for diabetic retinopathy and related eye diseases using\nretinal images from multiethnic populations with diabetes: A population-based study. Acta Ophthalmol. 98, e20–e28 (2020).\n16. Raman, R. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy using non-\nmydriatic retinal fundus images archived in electronic health records. Indian J. Ophthalmol. 68, 398–403 (2020).\n17. Takahashi, H. et al. Application of deep learning to the determination of diabetic retinopathy and glaucoma based on retinal\nfundus photographs. Jpn. J. Ophthalmol. 64, 368–375 (2020).\n18. Burlina, P. M. et al. Automated grading of age-related macular degeneration from color fundus images using deep convolutional\nneural networks. JAMA Ophthalmol. 138, 652–659 (2020).\n19. Huang, K., Zhang, L., Chen, Y., Xie, J. & Li, L. Saliency-guided self-supervised transformer for diabetic retinopathy grading. IEEE\nTrans. Med. Imaging[SPACE]https://doi.org/10.1109/TMI.2024.3045002 (2024).\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 14\nwww.nature.com/scientificreports/\n20. Thanikachalam, R., Sivakumar, M., Kalyani, M. & Kumar, H. Optimized deep convolutional neural networks for diabetic\nretinopathy and macular edema detection. Comput. Biol. Med.[SPACE]https://doi.org/10.1016/j.compbiomed.2024.105034\n(2024).\n21. Bodapati, S. & Balaji, K. Self-adaptive stacking ensemble with attention mechanisms for diabetic retinopathy severity prediction.\nArtif. Intell. Med.[SPACE]https://doi.org/10.1016/j.artmed.2024.101850 (2024).\n22. Bhati, S., Singh, P. & Thakur, R. Idanet: Interpretable dual attention network for diabetic retinopathy grading. IEEE\nAccess[SPACE]https://doi.org/10.1109/ACCESS.2024.3100089 (2024).\n23. Sivapriya, R., Chithra, M. & Ragavendran, C. Microvascular structure analysis in diabetic retinopathy classification using deep\nlearning. IEEE J. Biomed. Health Inform.[SPACE]https://doi.org/10.1109/JBHI.2024.3120031 (2024).\n24. Ohri, P. & Kumar, R. Su\n---CHUNK_BREAK---\npervised fine-tuned approach for diabetic retinopathy detection using transfer learning. J. Digit.\nImaging[SPACE]https://doi.org/10.1007/s10278-024-00609-1 (2024).\n25. Luo, Z., Wang, F. & Zheng, Q. Deep CNN model with multi-scale feature fusion for diabetic retinopathy detection. Expert Syst.\nAppl.[SPACE]https://doi.org/10.1016/j.eswa.2024.117845 (2024).\n26. Romero-Oraá, M. T., Martinez, J., Gonzalez, R. & Gonzalez, M. Attention-based framework for diabetic retinopathy grading in\nfundus images. Biomed. Signal Process. Control[SPACE]https://doi.org/10.1016/j.bspc.2024.105564 (2024).\n27. Zhang, Y., Wang, Y., Wu, X. & Li, J. Semi-supervised contrastive learning with saliency maps for diabetic retinopathy classification.\nPattern Recogn.[SPACE]https://doi.org/10.1016/j.patcog.2024.109164 (2024).\n28. Wong, L., Yang, H. & Zheng, Y. Transfer learning with feature-weighted ECOC ensembles for diabetic retinopathy grading.\nComput. Biol. Med.[SPACE]https://doi.org/10.1016/j.compbiomed.2023.105489 (2023).\n29. Bilal, A., Zhu, L., Deng, A., Lu, H. & Wu, N. Ai-based automatic detection and classification of diabetic retinopathy using u-net\nand deep learning. Symmetry 14, 1427. https://doi.org/10.3390/sym14071427 (2022).\n30. Bilal, A., Sun, G., Li, Y., Mazhar, S. & Khan, A. Diabetic retinopathy detection and classification using mixed models for a disease\ngrading database. IEEE Access[SPACE]https://doi.org/10.1109/ACCESS.2021.3056186 (2021).\n31. Bilal, A., Mazhar, S., Imran, A. & Latif, J. A transfer learning and u-net-based automatic detection of diabetic retinopathy from\nfundus images. Comput. Methods Biomech. Biomed. Eng. Imaging V i s u a l i z . [ S P A C E ] h t t p s: / / d o i . o r g/ 1 0 . 1 0 8 0 / 21 6 8 1 1 6 3 . 2 02 1 . 2 0 2 1 1 1\n1 (2022).\n32. Bilal, A., Sun, G., Mazhar, S. & Imran, A. Improved grey wolf optimization-based feature selection and classification using CNN\nfor diabetic retinopathy detection. 1–14 (2022).\n33. Bilal, A., Sun, G. & Mazhar, S. Diabetic retinopathy detection using weighted filters and classification using CNN. h t t ps : / / d o i . o rg /\n1 0 . 1 1 0 9 /C O N I T 5 1 4 8 0. 2 0 2 1 . 9 4 9 8 4 6 6 (2021).\n34. Bilal, A. et al. Improved support vector machine based on CNN-SVD for vision-threatening diabetic retinopathy detection and\nclassification. PLoS One 19, e0295951. https://doi.org/10.1371/journal.pone.0295951 (2024).\n35. Bilal, A., Liu, X., Baig, T., Long, H. & Shafiq, M. Edgesvdnet: 5g-enabled detection and classification of vision-threatening diabetic\nretinopathy in retinal fundus images. Electronics 12, 4094. https://doi.org/10.3390/electronics12194094 (2023).\n36. Bilal, A., Liu, X., Shafiq, M., Ahmed, Z. & Long, H. Nimeq-sacnet: A novel self-attention precision medicine model for vision-\nthreatening diabetic retinopathy using image data. Comput. Biol. Med. 171, 108099. h t t p s: / / d o i . o r g/ 1 0 . 1 0 1 6 / j. c o m p b i o m ed . 2 0 2 4 . 1\n0 8 0 9 9 (2024).\n37. Gupta, B. B., Gaurav, A. & Panigrahi, P. K\n---CHUNK_BREAK---\n. Analysis of security and privacy issues of information management of big data in B2B\nbased healthcare systems. J. Bus. Res. 162, 113859 (2023).\n38. Zaidan, A. A., AlSattar, H. A., Qahtan, S., Deveci, M. & Pamucar, D. Secure decision approach for internet of healthcare things\nsmart systems-based blockchain. IEEE Internet of Things Journal (2023).\n39. Zhou, Y., Song, L., Liu, Y. & Vijayakumar, P. A privacy-preserving logistic regression-based diagnosis scheme for digital healthcare.\nFutur. Gener. Comput. Syst. 144, 63–73 (2023).\n40. Singh, S. K. Linux yourself: Concept and programming 1st edn. (Chapman and Hall/CRC, Cham, 2021).\n41. Chui, K. T. et al. Multiround transfer learning and modified generative adversarial network for lung cancer detection. Int. J. Intell.\nSyst. 2023, 6376275 (2023).\n42. Hammad, M., Abd El-Latif, A. A., Hussain, A. & Abd El-Samie, F. E. Deep learning models for arrhythmia detection in IoT\nhealthcare applications. Comput. Electr. Eng. 100, 108011 (2022).\n43. Sutomo, H. I. Identification of organic and non-organic waste with computer image recognition using convolutionalneural\nnetwork with efficient-net-b0 architecture. J. Appl. Intell. Syst. 8(3), 320–330. https://doi.org/10.33633/jais.v8i3.9064 (2023).\n44. Y, V., Billakanti, N., Veeravalli, K., N, A. D. R. & Kota, L. Early detection of casava plant leaf diseases using efficientnet-b0. In 2022\nIEEE Delhi Section Conference (DELCON), 1–5, https://doi.org/10.1109/DELCON54057.2022.9753210 (2022).\n45. Agarwal, V. Complete architectural details of all efficientnet models. Medium (2021). h t t p s : / / t o w a r d s d at a s c i e n c e. c o m / c o m p le t e - a r\nc h i te c t u r a l - d et a i l s - o f - al l - e ffi c i en t n e t - m o d el s - 5 f d 5 b 7 36 1 4 2 .\n46. TanLikesMath. Diabetic retinopathy - resized data. Kaggle, https://www. kaggle.com/d atasets/tanl ikesmath/di abetic R-retinopathy,\nhttps: //d oi. or g/10. 13 14 0/RG. 2.2.1 3037.1 9 688-resized/data (20243).\n47. Yang, L. & Shami, A. On hyperparameter optimization of machine learning algorithms: Theory and practice. Neurocomputing 415,\n295–316. https://doi.org/10.1016/j.neucom.2020.07.061 (2020).\n48. Rathee, G., Saini, H., Garg, S., Choi, B. J. & Hassan, M. M. A secure data e-governance for healthcare application in cyber physical\nsystems. Int. J. Semantic Web Inf. Syst. 20, 1–17 (2024).\nAcknowledgement\nThis research work was funded by Institutional Fund Projects under grant no. (IFPIP-639-611-1443). Therefore,\nthe authors gratefully acknowledge technical and financial support from Ministry of Education and Deanship of\nScientific Research (DSR), King Abdulaziz University (KAU), Jeddah, Saudi Arabia.\nAuthor contributions\nL.A.: Conceptualization, methodology, writing original draft. S.K.S.: Formal analysis, supervision, reviewing\nand editing, data interpretation, supervision. S.K.: Conceptualization, methodology, Data curation, validation,\nwriting original draft, supervision. H.G.: Conceptualization, meth\n---CHUNK_BREAK---\nodology, writing original draft, Software, visu-\nalization, data interpretation. W.A.: Investigation, resources, manuscript preparation. V.A.: Methodology, valida-\ntion, writing – review and editing. S.B.: Data acquisition, software, writing – review and editing. K.T.C.: Formal\nanalysis, data interpretation, supervision. B.B.G.: Supervision, resources, project oversight, funding acquisition.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 15\nwww.nature.com/scientificreports/\nAdditional information\nCorrespondence and requests for materials should be addressed to S.K.S. or B.B.G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives\n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in\nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide\na link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have\npermission under this licence to share adapted material derived from this article or parts of it. The images or\nother third party material in this article are included in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence\nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this licence, visit h t t p :/ / c r e a t i v ec o m m o\nn s . o rg / l i c e n s e s/ b y - n c - n d / 4 . 0 / .\n© The Author(s) 2024\nScientific Reports | (2024) 14:30554 | https://doi.org/10.1038/s41598-024-81132-4 16"
}