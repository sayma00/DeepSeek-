{
  "Ensemble deep learning and EfficientNet for accurate diagnosis of diabetic retinopathy.pdf": "\n--- Page 1 ---\nEnsemble deep learning and \nEfficientNet for accurate diagnosis \nof diabetic retinopathy\nLakshay Arora1,12, Sunil K. Singh1,12, Sudhakar Kumar1,12, Hardik Gupta1,12, \nWadee Alhalabi2,12, Varsha Arya3,7,8,12, Shavi Bansal4,12, Kwok Tai Chui5,12 & \nBrij B. Gupta6,9,10,11,12\nDiabetic Retinopathy (DR) stands as a significant global cause of vision impairment, underscoring \nthe critical importance of early detection in mitigating its impact. Addressing this challenge head-on, \nthis study introduces an innovative deep learning framework tailored for DR diagnosis. The proposed \nframework utilizes the EfficientNetB0 architecture to classify diabetic retinopathy severity levels \nfrom retinal images. By harnessing advanced techniques in computer vision and machine learning, \nthe proposed model aims to deliver precise and dependable DR diagnoses. Continuous testing and \nexperimentation shows to the efficiency of the architecture, showcasing promising outcomes that \ncould help in the transformation of both diagnosing and treatment of DR. This framework takes \nhelp from the EfficientNet Machine Learning algorithms and employing advanced CNN layering \ntechniques. The dataset utilized in this study is titled ’Diagnosis of Diabetic Retinopathy’ and is \nsourced from Kaggle. It consists of 35,108 retinal images, classified into five categories: No Diabetic \nRetinopathy (DR), Mild DR, Moderate DR, Severe DR, and Proliferative DR. Through rigorous testing, \nthe framework yields impressive results, boasting an average accuracy of 86.53% and a loss rate \nof 0.5663. A comparison with alternative approaches underscores the effectiveness of EfficientNet \nin handling classification tasks for diabetic retinopathy, particularly highlighting its high accuracy \nand generalizability across DR severity levels. These findings highlight the framework’s potential \nto significantly advance the field of DR diagnosis, given more advanced datasets and more training \nresources which leads it to be offering clinicians a powerful tool for early intervention and improved \npatient outcomes.\nKeywords  Diabetic retinopathy, Deep learning, EfficientNet, CNN, Image dataset, Layering\nDiabetic retinopathy, a consequence of diabetes mellitus, manifests as alterations in the blood vessels within \nthe retina. This condition poses a significant threat to vision integrity and can culminate in blindness if not \nappropriately managed. The underlying pathology stems from prolonged exposure to elevated levels of glucose \nin the bloodstream, which gradually compromises the structural integrity of the retinal vasculature. Over time, \nthis vascular damage manifests as microaneurysms, hemorrhages, and exudates, which impair retinal function \nand compromise visual acuity1,2.\nThe progression of diabetic retinopathy is insidious, often characterized by an asymptomatic early phase. \nHowever, as the condition advances, symptoms such as blurred vision, floaters, and visual field deficits may \neme\n---CHUNK_BREAK---\nrge, signaling the need for prompt intervention. The hallmark of diabetic retinopathy lies in its propensity \nto induce macular edema and proliferative changes within the retina, further exacerbating visual impairment3.\n1Department of CSE, Chandigarh College of Engineering and Technology, Panjab University, Chandigarh, India. \n2Department of Computer Science, Immersive Virtual Reality Research Group, King Abdulaziz University, Jeddah, \nSaudi Arabia. 3Department of Electrical and Computer Engineering, Lebanese American University, Beirut 1102, \nLebanon. 4Insights2Techinfo, Jaipur, India. 5Hong Kong Metropolitan University (HKMU), Kowloon, Hong Kong. \n6Present address: Department of Computer Science and Information Engineering, Asia University, Taichung 413, \nTaiwan. 7Center for Interdisciplinary Research, University of Petroleum and Energy Studies (UPES), Dehradun, \nIndia. 8UCRD, Chandigarh University, Chandigarh, India. 9Kyung Hee University, 26 Kyungheedae-ro, Dongdaemun-\ngu, Seoul 02447, Korea. 10Symbiosis Centre for Information Technology (SCIT), Symbiosis International University, \nPune, India. 11University of Economics and Human Science, Warsaw, Poland. 12Lakshay Arora, Sunil K. Singh, \nSudhakar Kumar, Hardik Gupta, Wadee Alhalabi, Varsha Arya, Shavi Bansal, Kwok Tai Chui and Brij B. Gupta \ncontributed equally to this work. email: sksingh@ccet.ac.in; bbgupta@asia.edu.tw\nOPEN\nScientific Reports |        (2024) 14:30554 \n1\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports\n\n---CHUNK_BREAK---\n\n--- Page 2 ---\nRoutine ophthalmic examination plays a pivotal role in the early detection and management of diabetic \nretinopathy. Screening modalities such as fundus photography and optical coherence tomography enable \nclinicians to identify subtle retinal changes before symptomatic onset. Timely intervention is imperative to \nmitigate disease progression and preserve visual function4.\nThe detection of diabetic retinopathy is often aided by identifying specific retinal lesions. One such lesion, \na microaneurysm, appears as a small, round dot on the retina during the initial phases of the disease. These \ndots have well-defined edges and are typically no more than 125 micrometers in diameter. Although there are \nsix distinct types of microaneurysms, treatment approaches remain consistent across all types. Another lesion \nlinked to diabetic retinopathy is the haemorrhage, which presents as a large, irregularly shaped spot on the \nretina, generally exceeding 125 micrometers in size. Additional lesions, known as hard exudates, result from \nplasma leakage and show up as yellow spots with defined borders on the retina. Soft exudates, caused by swelling \nof nerve fibers, take the form of white, oval patches on the retina, as illustrated in Fig. 1.\nThe International Clinical Diabetic Retinopathy Disease Severity Scale (ICDRDSS), introduced by the \nGlobal Diabetic Retinopathy Project Group, is a pivotal classification system designed fo\n---CHUNK_BREAK---\nr the assessment and \ncategorization of diabetic retinopathy (DR).The ICDRDSS comprises five distinct severity levels, each capturing \nthe progression of the disease and facilitate accurate diagnosis and treatment planning.\nDR progression stages are characterized by different disease severity levels. Figure 2 shows some of the grades \nfor Diabetic Retinopathy The findings associated with each grade are as seen in Table 1.\nEach stage of diabetic retinopathy is characterized by unique characteristics and properties, which can \npose challenges for doctors when making a diagnosis. According to a study by Google, the stages of diabetic \nretinopathy may prove difficult to accurately evaluate manually even by well-trained clinicians. So, the concept \nof automatic detection has garnered attention as a means to enhance accuracy and efficiency in the diagnosis of \nDR.\nThe predominant trend in analyzing various models for artificial intelligence (AI) implementation reveals \na reliance on six key constituent AI models, all of which are deeply rooted in the domain of machine learning. \nThese models are CNN, ANN, Other-NN, Fuzzy ML, SVM, RF (Random Forest). This study integrates the \npowerful EfficientNet Framework with the Layering techinques in CNN to build a model that would help \nFig. 2.  Various grade illustrations6.\n \nFig. 1.  Illustration of diabetic retinopathy lesions5.\n \nScientific Reports |        (2024) 14:30554 \n2\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n--- Page 3 ---\nincrease the accuracy for detection of DR. By leveraging CNN layering techniques and compound scaling in \nEfficientNet, our framework optimally balances performance and computational resources. Through rigorous \nexperimentation, we demonstrate how this model achieves high accuracy in DR classification while providing \ninterpretable outputs that highlight relevant retinal regions. This advancement aims to enhance the practical \napplication of automated DR diagnosis, bridging the gap between model accuracy and clinical interpretability.\nEfficientNet’s central concept is compound scaling, which systematically adjusts network dimensions-such \nas depth, width, and resolution-to enhance performance. This method ensures optimal resource allocation, \nallowing the model to achieve high accuracy while remaining resource-efficient across various tasks and \noperational constraints7,8.\nThe various key contributions in this research article are given as follows:\n•\t The authors conducted an extensive review of existing literature on diabetic retinopathy (DR), focusing on \nits pathology, impact on vision, and the critical importance of early detection to prevent vision impairment.\n•\t They contributed significantly to the conceptualization and design of an innovative deep learning framework \ntailored to DR diagnosis, integrating the EfficientNet architecture and CNN layering techniques to enhance \ndiagnostic accuracy.\n•\t The authors collaborated in selecting, p\n---CHUNK_BREAK---\nreparing, and preprocessing the Kaggle dataset, which comprises \n35,108 retinal images categorized across various severity levels of DR, ensuring comprehensive data coverage \nfor training and validation.\n•\t The authors worked collectively in writing and editing the introduction, providing a clear and structured \noverview of the problem statement, the clinical relevance of the study, and the potential impact of the pro­\nposed solution on DR diagnosis.Further the remaining paper comprises of the following sections. “Literature \nreview” Section of the research includes the Literature Review and contains all the previous recent work done \nin the field. “Preliminary theory” Section of the research includes the background of the framework and the \ntheoretical information regarding the concepts used. “Proposed framework” Section introduces the proposed \nFramework followed by the experimental setup for the research. “Experimental setup” Section includes the \nresults of the research. “Experimental results” Section discusses the results and describes the limitations in \nthe research. “Discussions and limitations” Section of the research includes the conclusion and future scope \nin the present research.\nLiterature review\nDeep learning techniques have shown promising results in the automated diagnosis of diabetic retinopathy, \na significant cause of blindness globally. Multiple studies have contributed to advancements in this field, \ndemonstrating the effectiveness of deep learning models in accurately detecting diabetic retinopathy from \nretinal fundus photographs.\nThe work of Gulshan et al.9 laid a critical foundation, presenting a robust deep learning algorithm capable \nof accurately identifying diabetic retinopathy in retinal fundus photographs. Building on this, Ting et al.10 \ndeveloped a system that incorporated retinal images from multiethnic populations with diabetes, emphasizing the \nimportance of diversity in dataset design for improved model generalization. Abràmoff et al.11 further validated \nthe clinical applicability of autonomous AI-based diagnostic systems through a pivotal trial, showcasing their \npotential in real-world primary care environments and facilitating early intervention. Subsequent research efforts \nfocused on enhancing the practical deployment of these models. Li et al.12 introduced an automated grading \nsystem designed to identify vision-threatening diabetic retinopathy cases, aiming to streamline urgent referrals \nbased on disease severity. In a similar vein, Bellemo et al.13 conducted clinical validation in African healthcare \nsettings37, highlighting how deep learning models can offer accessible and accurate diagnostic tools to address \nhealthcare disparities38. Bhaskaranand et al.14 also contributed to this area by developing an automated screening \nand monitoring system to facilitate timely detection and management of diabetic retinopathy progression.\nFurther studies have underscored the importance of generalizability and \n---CHUNK_BREAK---\nadaptability in model development. \nSahlsten et al.15 conducted a population-based study, emphasizing the necessity of incorporating diverse \npopulation data in model training to enhance robustness. Meanwhile, Raman et al.16 addressed the challenges \nof training deep learning models on non-mydriatic retinal fundus images from electronic health records39, \nenhancing their applicability to real-world clinical datasets. Takahashi et al.17 extended the scope of deep \nlearning applications to include both diabetic retinopathy and glaucoma diagnosis, showcasing the potential \nfor multifaceted disease detection. Burlina et al.18, although focused on age-related macular degeneration, \ndemonstrated the versatility of deep learning models in analyzing retinal images across various ophthalmic \nGrade\nDescription\nGrade 0\nNo DR - No observable indicators of diabetic retinopathy.\nGrade 1\nMild NPDR - Small, circular microaneurysms appear on the retina.\nGrade 2\nModerate NPDR - Changes beyond microaneurysms are present, though the condition has not progressed to severe NPDR.\nGrade 3\nSevere NPDR - Any of the following signs are present, without signs of proliferative DR: over 20 intraretinal hemorrhages \nin all four quadrants, evident venous beading in at least two quadrants, or pronounced intraretinal microvascular \nabnormalities (IrMA) in one or more quadrants.\nGrade 4\nProliferative DR - Characterized by the presence of neovascularization, vitreous or preretinal hemorrhages, or both.\nTable 1.  International clinical diabetic retinopathy disease severity scale (ICDRDSS)5.\n \nScientific Reports |        (2024) 14:30554 \n3\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 4 ---\nconditions. Bilal et al.29 developed an AI-based system utilizing U-Net and deep learning to automatically detect \nand classify diabetic retinopathy in fundus images, emphasizing the effectiveness of U-Net architectures in image \nsegmentation tasks critical for disease classification. Building on these efforts, Bilal et al.30 used a combination of \nmixed models for classifying diabetic retinopathy severity, showcasing the adaptability of ensemble approaches \nin handling complex disease grading tasks.\nFurther enhancing the detection framework, Bilal et al.31 incorporated transfer learning techniques with \nU-Net, demonstrating an improved model for diabetic retinopathy detection. Their methodology highlighted \nthe role of pretrained models in efficiently capturing essential retinal features from fundus images. Huang \net al.19 introduced a saliency-guided self-supervised transformer called “Ssit,” which enhances diabetic \nretinopathy grading through focused attention on relevant retinal areas, leveraging saliency maps for improved \nfeature extraction. Validated on large datasets, this model demonstrates significant improvements in grading \naccuracy. Thanikachalam et al.20 built on this approach by proposing an optimized deep CNN for detect\n---CHUNK_BREAK---\ning \ndiabetic retinopathy and diabetic macular edema, incorporating adaptive learning techniques to achieve high \nclassification accuracy. Bodapati and Balaji21 further innovated by developing a self-adaptive stacking ensemble \nmodel that combines multiple neural networks with attention mechanisms, yielding high accuracy across \ndifferent imaging conditions.\nAdvancements in interpretability have also been a focal point in recent studies. Bhati et al.22 presented IDANet, \nan interpretable dual attention network40 that enhances diabetic retinopathy grading by focusing on critical retinal \nregions, maintaining diagnostic accuracy while providing clinical interpretability. Sivapriya et al.23 introduced a \nmodel that classifies diabetic retinopathy by emphasizing microvascular structures, highlighting the importance \nof retinal structure analysis. Ohri and Kumar24 further improved detection through a supervised fine-tuned \nmodel, leveraging transfer learning for efficient DR identification. In exploring optimization techniques, Bilal et \nal.32 applied Grey Wolf Optimization with CNN models, enhancing feature selection processes and improving \nclassification accuracy, which is pivotal for early diabetic retinopathy intervention. Another approach from \nBilal et al.33 combined CNNs with weighted filters and adaptive filtering, underscoring the importance of noise \nreduction and data augmentation for accurate classification, especially in retinal image processing.\nA significant contribution by Bilal et al.34 leveraged a CNN-SVD-enhanced Support Vector Machine for \ndetecting vision-threatening diabetic retinopathy, marking a shift toward hybrid models that integrate machine \nlearning with deep learning for robust detection capabilities. Additionally, Bilal et al.35 introduced EdgeSVDNet, \na 5G-enabled framework for diagnosing diabetic retinopathy in real time, which could greatly enhance \naccessibility and speed in remote medical diagnosis. Bilal et al.36 proposed the NIMEQ-SACNet model using \nself-attention mechanisms, tailored for precision medicine applications in diabetic retinopathy. This advanced \nmodel integrates self-attention with CNNs to improve accuracy and adaptability, demonstrating the potential for \npersonalized diagnostic tools in precision medicine frameworks. Luo et al.25 proposed a CNN model that captures \nboth local and long-range dependencies in retinal images, improving classification across diabetic retinopathy \nstages through enhanced representation of retinal features. Romero-Oraá et al.26 introduced an attention-based \nframework to isolate relevant features for DR grading, demonstrating optimized focus in model design. Zhang \net al.27 explored a semi-supervised contrastive learning method, incorporating saliency maps and unlabeled \ndata for improved robustness. Finally, Wong et al.28 utilized transfer learning41 with parameter optimization, \ndemonstrating the efficacy of feature-weighted ECOC ensembles in enhancing diabetic retino\n---CHUNK_BREAK---\npathy diagnostics.\nThese studies collectively are shown in Tables 2, 3 and 4 highlight the advancements in deep learning \ntechniques for diabetic retinopathy diagnosis, emphasizing the importance of early detection and intervention \nin preventing vision loss. The proposed deep learning framework in this paper builds upon and contributes to \nthis body of research by leveraging EfficientNet as the base model and incorporating advanced CNN layering to \nenhance model effectiveness.\nResearch gap\nWhile the existing body of research in diabetic retinopathy (DR) diagnosis has made considerable progress \nusing deep learning techniques, a significant research gap persists in leveraging advanced architectures to \nmaximize diagnostic accuracy and efficiency. Previous studies have largely focused on utilizing architectures \nsuch as Inception, ResNet, and DenseNets for DR detection, which have shown promising results in terms of \naccuracy. However, there remains an opportunity to explore novel architectures that can deliver higher precision \nand robustness in classifying DR severity levels. This study aims to address this gap by introducing a framework \nbased on the EfficientNetB0 architecture, designed specifically to improve diagnostic accuracy and reliability \nfor DR severity classification. Through extensive testing and validation, the proposed model seeks to bridge \nthe existing gap by providing a dependable solution that achieves high accuracy rates and effectively supports \nclinical decision-making in DR diagnosis.\nPreliminary theory\nEfficientNet\nEfficientNet is a family of convolutional neural network (CNN) models42 appreciated for their efficiency and \neffectiveness in image classification tasks. The architecture of EfficientNet models is characterized by a stem, \nfollowed by multiple blocks, each containing sub-blocks. The overall architecture is composed of five modules, \nwhich are combined to create the EfficientNet models43,44. These modules can be seen in Figure 3.\nStem: The initial portion of the network, the stem, is where the foundation for subsequent layers is set. The \ninput image is processed here to extract basic features.\nBlocks: EfficientNet-B0 is comprised of seven blocks, with each contributing to the hierarchical feature \nextraction process. These blocks play a crucial role in the learning of increasingly abstract features as information \nflows through the network.\nScientific Reports |        (2024) 14:30554 \n4\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 5 ---\nS. \nno.\nReferences\nMethodologies\nData source\nAdvantages\nDisadvantages and gaps\n11\nHuang et al.19\nSelf-supervised transformer with saliency \nmaps for improving model attention and \ngrading accuracy\nPublicly \navailable \nfundus image \ndatasets\nEnhanced model attention and \ninterpretability through saliency maps\nHigh complexity due to transformer-based \narchitecture, may require extensive computation \nresources\n12\nThani\n---CHUNK_BREAK---\nkachalam \net al.20\nOptimized CNN with adaptive learning \nand hyperparameter tuning\nFundus \nimages \nfrom open \nrepositories\nEfficient in resource usage and shows \nimproved accuracy with adaptive learning\nLimited interpretability, potential overfitting \ndue to CNN’s static feature extraction \ncapabilities\n13\nBodapati & \nBalaji21\nEnsemble of attention-based neural \nnetworks with stacking for increased \nrobustness\nProprietary \nDR dataset\nHigh robustness and accuracy, effective in \nhandling severity prediction\nMay suffer from longer training times and \nresource requirements due to stacking and \nensemble strategies\n14\nBhati et al22\nDual attention network focusing on critical \nretinal regions for interpretability\nDR fundus \nimages from \nlarge-scale \ndatasets\nImproved interpretability with attention \non specific retinal regions, higher \naccuracy in grading\nComplex architecture may be computationally \nintensive; dependency on quality of retinal \nimages\n15\nSivapriya et \nal23\nDeep learning model analyzing \nmicrovascular structures in fundus images\nPublic DR \nfundus image \ndatasets\nFocuses on fine-grained features, \npotentially enhancing detection \nsensitivity\nLimited scalability, may struggle with low-\nquality images where microvascular structures \nare not clear\n16\nOhri & \nKumar24\nSupervised learning with transfer learning, \nfine-tuning on CNN models for improved \nDR detection accuracy\nNot Specified\nHigh accuracy due to supervised fine-\ntuning and transfer learning; improved \nmodel performance on labeled DR data\nDependency on large, labeled datasets; limited \ninterpretability due to black-box nature of CNN; \nlacks attention mechanisms for feature focus\n17\nLuo et al.25\nDeep CNN with local and global retinal \nfeatures using long-range dependency \nmodeling\nRetinal \nFundus \nImages\nEffective in capturing both fine-grained \nand global retinal features for accurate \nDR stage classification\nHigh computational cost due to deep CNN \nlayers; may require extensive preprocessing of \nimages for better feature extraction\n18\nRomero-Oraá \net al.26\nAttention mechanisms isolate relevant \nfeatures, focusing model on key retinal \nregions for grading\nFundus Image \nDataset\nImproved interpretability by visualizing \nfeature importance; focuses on critical \nareas, enhancing decision accuracy\nLimited to fundus images; potential over-\nreliance on salient features, may overlook subtle \nsigns; lacks robustness across diverse datasets\n19\nZhang et al.27\nSemi-supervised learning with contrastive \napproach, uses saliency maps for robust \ngrading\nNot Specified\nReduces dependency on large labeled \ndatasets; robust against noisy data due to \ncontrastive learning\nComplexity in implementing contrastive \nlearning; risk of lower performance without \naccurate saliency maps for guidance\n20\nWong et al.28\nTransfer learning optimized with feature-\nweighted Error-Correcting Output Codes \n(ECOC) for ensemble grading\nDiverse DR \nImage Dataset\nHigh accuracy due to parameter \noptimization and ensemble appro\n---CHUNK_BREAK---\nach; \nfeature-weighted ECOC enhances \nclassification\nComplexity in hyperparameter tuning; \npotentially high resource requirements due \nto ensemble approach and ECOC processing \ncomplexity\nTable 3.  Related works in diabetic retinopathy diagnosis part - 2.\n \nS. \nno.\nReferences\nMethodologies\nData source\nAdvantages\nDisadvantages and gaps\n1\nGulshan et al.9\nCNN-based deep learning model for \nbinary DR classification\nRetinal fundus images \nfrom EyePACS dataset\nDemonstrated high sensitivity and \nspecificity in a clinical setting\nLimited to binary classification (DR vs. \nno DR); lacks granularity in severity \nlevels and interpretability\n2\nTing et al.10\nCNNs trained on multiethnic retinal \nimages, with system fine-tuned for DR \nseverity prediction\nRetinal images from \nmultiethnic EyePACS, \nUS and Singapore \ndatasets\nEnhanced generalizability across \nethnicities and improved accuracy\nHigh computational demand; limited \nexplainability for clinical use\n3\nAbràmoff et \nal.11\nAutonomous AI model with \nconvolutional neural networks for DR \ndiagnosis\nEyePACS data from \nprimary care settings\nHigh applicability in clinical settings \nwith autonomous operation\nFocuses on binary classification; does \nnot provide insights into DR severity \ngrading\n4\nLi et al.12\nMulti-stage CNN framework targeting \nvision-threatening DR\nLarge dataset of color \nfundus photographs\nSpecialized in detecting severe DR \ncases, improving triaging\nLimited interpretability and \ngeneralization due to specialized target\n5\nBellemo et al.13\nAI-based CNN model tailored for DR \nscreening in low-resource settings\nRetinal fundus images \nfrom African clinical \nsettings\nValidated model efficacy in diverse, \nresource-limited regions\nModel scalability is constrained; \nlimited interpretability for practical \ndiagnostic insights\n6\nBhaskaranand \net al.14\nDeep learning-based automated screening \nand monitoring system for DR\nRetinal fundus images\nOffers continuous monitoring of DR \nprogression, supports early detection\nLimited interpretability in decision-\nmaking, lacking an advanced \nexplanation-guided method\n7\nSahlsten et al.15\nDeveloped and validated DL algorithms \nfor DR on a large, diverse population\nMultiethnic population \ndata\nHigh robustness due to diverse \npopulation data, ensuring \ngeneralizability\nNo focused methodology for \ninterpretability, which can limit \nclinician trust\n8\nRaman et al.16\nDeveloped DL models on non-mydriatic \nimages to detect DR\nElectronic health records\nSupports diagnosis using non-\nmydriatic images, allowing more \naccessible and frequent testing\nLimited attention mechanisms, \ninterpretability concerns\n9\nTakahashi et \nal.17\nDual-purpose model assessing both DR \nand glaucoma\nRetinal fundus \nphotographs\nSupports multi-disease diagnosis, \nenhancing model utility in broader \nophthalmology\nLacks targeted grading for different \nDR severity stages, gaps in explaining \nresults for individual conditions\n10\nBurlina et al.18\nConvolutional Neural Networks for \nautomated grading of age-related\n---CHUNK_BREAK---\n macular \ndegeneration (AMD)\nColor fundus images\nHigh accuracy in AMD grading, shows \npotential for adaptation to related \nconditions such as DR\nFocus on AMD limits direct \napplicability to DR, interpretability and \nDR severity grading are not addressed\nTable 2.  Related works in diabetic retinopathy diagnosis part - 1.\n \nScientific Reports |        (2024) 14:30554 \n5\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 6 ---\nSub-blocks: Within each block, there are varying numbers of sub-blocks. Specific operations are performed \nby each sub-block to transform the input features.\nModules:\n•\t Module 1: Serving as the starting point for the first sub-block in the first block.\n•\t Module 2: Acting as the starting point for the first sub-block in all blocks except the first one.\n•\t Module 3: Skip connections are established to all sub-blocks, facilitating information flow and aiding in gra­\ndient propagation.\n•\t Module 4: Utilized for combining skip connections in the first sub-blocks, thereby enhancing feature rep­\nresentation.\n•\t Module 5: Connecting each sub-block to its preceding sub-block via skip connections and combining them to \nrefine feature maps.Sub-block Types:\n•\t\n•\t Sub-block 1: Exclusive to the first sub-block in the first block, initializing the feature extraction process.\n•\t Sub-block 2: Employed as the first sub-block in subsequent blocks, contributing to feature refinement.\n•\t Sub-block 3: Utilized for all sub-blocks except the first one in each block, further enhancing feature rep­\nresentation.By combining these modules and sub-blocks in a specific manner, a balance between model size, \ncomputational cost, and performance is achieved by EfficientNet-B0. It is noteworthy that EfficientNet-B0 \ncomprises 237 layers. Thus, EfficientNet models excel in various image classification tasks while remaining \ncomputationally efficient. The basic architecture for the EfficientNet Model can be in Fig. 4.\nFig. 3.  Types of modules45.\n \nS. \nno.\nReferences\nMethodologies\nData source\nAdvantages\nDisadvantages and gaps\n21\nBilal et al.29\nAI-based automatic detection and classification \nusing U-Net and deep learning\nRetinal fundus images\nHighly effective in image segmentation \nfor disease classification\nLimited to dataset characteristics, \nlacking generalizability across diverse \npopulations\n22\nBilal et al.30\nMixed models for disease grading and severity \nclassification\nDiabetic Retinopathy \nGrading Database\nImproved adaptability of ensemble \nmodels for complex grading\nMixed models may increase \ncomputational complexity\n23\nBilal et al.31\nTransfer learning with U-Net for enhanced \ndetection accuracy\nRetinal fundus images\nEfficient feature extraction with \npretrained models\nTransfer learning limited by domain \nspecificity\n24\nBilal et al.32\nGrey Wolf Optimization with CNN for feature \nselection\nRetinal images\nEnhanced feature selection and \nclassification accuracy\nOptimization technique may not \ngenera\n---CHUNK_BREAK---\nlize well to all data types\n25\nBilal et al.33\nCNNs with weighted filters and adaptive \nfiltering for classification\nRetinal fundus images\nEffective noise reduction and improved \nclassification accuracy\nIncreased model complexity and \ntraining time\n26\nBilal et al.34\nCNN-SVD-enhanced SVM for detecting \nvision-threatening retinopathy\nRetinal fundus images\nRobust detection capabilities through \nhybrid model\nComplex hybrid structure may require \nsignificant computational resources\n27\nBilal et al.35\nEdgeSVDNet, 5G-enabled for real-time \ndiagnosis\nRetinal fundus images \nwith 5G connectivity\nEnhanced accessibility and speed for \nremote diagnostics\nDependent on 5G infrastructure, limited \nin areas without high-speed connectivity\n28\nBilal et al.36\nNIMEQ-SACNet model with self-attention for \nprecision medicine\nRetinal image data for \nprecision diagnostics\nHigh accuracy and adaptability for \nprecision medicine applications\nComplexity of self-attention mechanism \nmay increase model size and training \nrequirements\nTable 4.  Related works in diabetic retinopathy diagnosis part - 3.\n \nScientific Reports |        (2024) 14:30554 \n6\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 7 ---\nHyperparameters\nHyperparameters are pivotal components in machine learning and deep learning algorithms, as they govern \nthe learning process and influence the resultant model parameters. Distinguished by the prefix ’hyper_’, \nthese parameters serve as high-level controls that shape the learning trajectory and ultimately determine the \ncharacteristics of the trained model.\nUnlike model parameters, which are learned during the training process and directly influence the model’s \npredictions, hyperparameters remain external to the resulting model. They are integral to the learning algorithm’s \nfunctionality, yet they do not become ingrained within the model structure.\nSome notable examples of hyperparameters encompass a diverse array of settings that profoundly impact the \nlearning process and model performance:\n•\t Train-Test Split Ratio: Dictates the proportion of data allocated for training versus testing, influencing the \nmodel’s ability to generalize to unseen data.\n•\t Learning Rate: A crucial parameter in optimization algorithms like gradient descent, controlling the magni­\ntude of parameter updates during training.\n•\t Choice of Optimization Algorithm: Determines the approach used to minimize the model’s loss function, with \noptions including gradient descent, stochastic gradient descent, or advanced techniques like the Adam opti­\nmizer.\n•\t Choice of Activation Function: Pertains to the nonlinear transformation applied within neural network lay­\ners, with popular functions including Sigmoid, ReLU, and Tanh, impacting the network’s capacity to capture \ncomplex patterns.\n•\t Cost or Loss Function: Defines the objective function optimized during training, guiding the model towards \nminimizing prediction errors or maximizing \n---CHUNK_BREAK---\nperformance metrics.\n•\t Number of Hidden Layers and Activation Units: Crucial architectural decisions in neural networks, influenc­\ning the network’s depth, breadth, and expressive capacity.\n•\t Dropout Rate: Specifies the probability of randomly dropping neurons during training, a regularization tech­\nnique aimed at preventing overfitting.\n•\t Number of Training Iterations (Epochs): Specifies how many times the model will pass through the full training \ndataset, influencing both convergence rate and model stability.\n•\t Number of Clusters: Applicable in clustering tasks, this parameter impacts the detail and organization of the \nclusters formed.\n•\t Kernel or Filter Size in Convolutional Layers: Determines the scope of convolution operations, essential for \ncapturing features from input data within convolutional neural networks.\n•\t Pooling Size: Sets the dimensions of pooling areas in CNNs, affecting the downsampling of features and the \ndevelopment of spatial hierarchies.\n•\t Batch Size: Defines the quantity of samples processed per training iteration, impacting both computational \nefficiency and the accuracy of gradient estimation.By judiciously tuning these hyperparameters, practitioners \ncan optimize the learning process, enhance model performance, and achieve superior results across a diverse \nrange of machine learning and deep learning tasks. Table 5 gives a summary for Hyperparameters for different \nMachine Learning Algorithms\nFlatten\nThe Flatten method within the layers module is considered a crucial tool for reshaping data, particularly when \ntransitioning from convolutional layers to fully connected layers in a neural network. This method is essentially \nFig. 4.  Architecture for EfficientNet-B045.\n \nScientific Reports |        (2024) 14:30554 \n7\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 8 ---\nutilized to collapse or flatten the input tensor into a one-dimensional tensor, which is deemed essential for it to \nbe fed into a dense layer.\nWhen dealing with convolutional layers, the output typically comprises a multi-dimensional tensor, where \neach dimension represents different features extracted by the convolutional filters. However, fully connected \nlayers require a one-dimensional input, where each element corresponds to a single feature. This is where the \nflatten method proves to be handy.\nBelow are few of the most important features for flatten:\n•\t Input Tensor: A multi-dimensional tensor, typically the output of a convolutional layer, serves as the starting \npoint.\n•\t Flattening Operation: The flatten method reshapes the tensor by concatenating all the elements along all di­\nmensions except the batch dimension.\n•\t Output Tensor: The result is a one-dimensional tensor that can be directly fed into a dense layer.Figure 5 rep­\nresents and displays a standard Flatten Layer with inter-layer connections.\nDense\nA dense layer, also referred to as a fully connected layer, is a fund\n---CHUNK_BREAK---\namental component in neural network \narchitectures, especially in feedforward networks. In this layer, each neuron is linked to every neuron in the \nsubsequent layer, enabling comprehensive connectivity between layers.\nIn TensorFlow, adding a dense layer to a neural network model requires specifying the number of neurons or \nunits in that layer, which defines the output space’s dimensionality. Each neuron in a dense layer receives input \nfrom all neurons in the preceding layer, and it computes an output by applying a weighted sum to these inputs, \nfollowed by an activation function. Figure 6 illustrates the typical structure of a dense layer.\nThe parameters of a dense layer include weights and biases. The strength of the connections between neurons \nis represented by the weights, and offsets are learned by the model through biases to better fit the data.\nDense layers are versatile and can be used in various neural network architectures for tasks such as \nclassification, regression, and even unsupervised learning. They’re often stacked together with activation \nfunctions like ReLU (Rectified Linear Unit) or sigmoid to introduce non-linearity into the model, enabling it to \nlearn complex patterns and relationships in the data.\nML algorithm\nMain HPs\nOptional HPs\nHPO methods\nLibraries\nRidge & lasso\nAlpha\n–\nBO-GP\nSkopt\nLogistic regression\nPenalty, c, solver\n–\nBO-TPE, SMAC\nHyperopt, SMAC\nKNN\nn_neighbors\nWeights, p, algorithm\nBOs, Hyperband\nSkopt, Hyperopt, \nSMAC, Hyperband\nSVM\nC, kernel, epsilon (for SVR)\nGamma, coef0, \ndegree\nBO-TPE, SMAC, \nBOHB\nHyperopt, SMAC, \nBOHB\nNB\nAlpha\n–\nBO-GP\nSkopt\nDT\nCriterion, max_depth, min_samples_split, min_samples_leaf, max_features, splitter, \nmin_weight_fraction_leaf, max_leaf_nodes\n–\nGA, PSO, BO-TPE, \nSMAC, BOHB\nTPOT, Optunity, \nSMAC, BOHB\nRF & ET\nn_estimators, max_depth, criterion, min_samples_split, min_samples_leaf, max_\nfeatures, splitter, min_weight_fraction_leaf, max_leaf_nodes\n–\nGA, PSO, BO-TPE, \nSMAC, BOHB\nTPOT, Optunity, \nSMAC, BOHB\nXGBoost\nn_estimators, max_depth, learning_rate, subsample, colsample_bytree, min_child_\nweight, gamma, alpha, lambda\n–\nGA, PSO, BO-TPE, \nSMAC, BOHB\nTPOT, Optunity, \nSMAC, BOHB\nVoting\nEstimators, voting weights\n–\nGS\nSklearn\nBagging\nBase_estimator, n_estimators\nmax_samples, \nmax_features\nGS, BOs\nSklearn, Skopt, \nHyperopt, SMAC\nAdaBoost\nBase_estimator, n_estimators, learning_rate\n–\nBO-TPE, SMAC\nHyperopt, SMAC\nDeep learning\nNumber of hidden layers, ‘units’ per layer, loss, optimizer, Activation, learning_rate, \ndropout rate, epochs, batch_size, early stop patience, number of frozen layers (if \ntransfer learning is used)\n–\nPSO, BOHB\nOptunity, BOHB\nHierarchical \nclustering\nn_clusters, distance_threshold\nLinkage\nBOs, Hyperband\nSkopt, Hyperopt, \nSMAC, Hyperband\nDBSCAN\neps, min_samples\n–\nBO-TPE, SMAC, \nBOHB\nHyperopt, SMAC, \nBOHB\nGaussian mixture\nn_components\ncovariance_type, \nmax_iter, tol\nBO-GP\nSkopt\nPCA\nn_components\nsvd_solver\nBOs, Hyperband\nSkopt, Hyperopt, \nSMAC, Hyperband\nLDA\nn_compon\n---CHUNK_BREAK---\nents\nsolver, shrinkage\nBOs, Hyperband\nSkopt, Hyperopt, \nSMAC, Hyperband\nTable 5.  Summary of hyperparameters and hyperparameter optimization (HPO) methods for various machine \nlearning algorithms.\n \nScientific Reports |        (2024) 14:30554 \n8\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 9 ---\nProposed framework\nThe proposed framework leverages the EfficientNetB0 model to categorize the severity levels of diabetic \nretinopathy from retinal images. EfficientNetB0 was chosen due to its exceptional accuracy in image classification \ntasks and its computational efficiency, achieved via a compound scaling technique that optimally adjusts the \nmodel’s depth, width, and resolution. In this framework, the upper layers of a pre-trained EfficientNetB0 \nmodel (pre-trained on ImageNet) were removed by setting include_top=False, and a new dense layer with five \nneurons was incorporated to classify images into five distinct diabetic retinopathy severity categories. A softmax \nactivation function is applied to the final layer to produce probability distributions across these classes8.\nThe methodology employs an algorithmic sequence: preprocessing starts by scaling pixel values to the [0,1] \nrange and implementing data augmentation strategies like random horizontal flips, rotations, and zooms. \nThe EfficientNetB0 model, initialized with ImageNet weights, is then enhanced by adding a flatten layer and a \ndense layer with softmax activation for classification purposes. Training is fine-tuned with a custom learning \nrate scheduler and early stopping triggered by validation loss, leading to a model that is both accurate and \ncomputationally efficient. This holistic framework delivers optimal performance in diabetic retinopathy classi­\nfication. \nFig. 6.  Standard dense layer.\n \nFig. 5.  Standard flatten layer.\n \nScientific Reports |        (2024) 14:30554 \n9\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n--- Page 10 ---\nAlgorithm 1.  EfficientNet-Based Model for Diabetic Retinopathy Detection\nModel architecture\nThe EfficientNetB0 model was chosen for this study due to its proven high performance and computational \nefficiency in image classification tasks. It employs a compound scaling strategy that balances the network’s \ndepth, width, and resolution, ensuring both accuracy and efficiency. In this setup, the top layers of the pre-\ntrained EfficientNetB0 (trained on ImageNet) were removed (include_top=False), and a new dense layer with \nfive neurons was added, each representing a diabetic retinopathy severity level. The softmax activation function \nin this final layer generates probability distributions across these classes. The model’s total parameter count was \n4,049,571, of which 4,007,548 were trainable, enabling efficient learning from the retinal images while keeping \ncomputational demands low.\nThe training process used the Adam optimizer for its adaptability in lea\n---CHUNK_BREAK---\nrning rates. Additionally, a custom \nlearning rate scheduler was applied, reducing the rate by half every three epochs to mitigate overfitting. Sparse \nFig. 7.  Proposed algorithm flow for EfficientNetB0.\n \nScientific Reports |        (2024) 14:30554 \n10\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n--- Page 11 ---\nCategorical Cross-Entropy was employed as the loss function, suitable for multi-class classification tasks. The \nmodel was trained over 40 epochs with an initial learning rate of 0.001 and a batch size of 32, utilizing data \naugmentation techniques like random flips, rotations, and zooms. Figure 7 illustrates the model’s flow and \nprovides a basic architectural overview of the proposed solution.\nExperimental setup\nThe experimental setup was designed with a focus on improving model generalization through data augmentation \nand careful tuning of hyperparameters. Data augmentation was applied using Keras’ ImageDataGenerator, which \nincluded horizontal flipping, random rotations of 0.1 radians, and random zooming. The model was trained with \na batch size of 32, using images resized to 224 × 224 pixels. A total of 40 epochs were completed, yielding an \naccuracy of 0.8653 and a loss of 0.5663, as observed in the training logs.\nDataset description\nThe dataset utilized in this study is sourced from Kaggle46. It consists of 35,108 retinal images, classified into \nfive categories: No Diabetic Retinopathy (DR), Mild DR, Moderate DR, Severe DR, and Proliferative DR. \nThe distribution of images is highly imbalanced, with 25,802 images labeled as ’No DR’, 5288 as ’Mild DR’, \n2438 as ’Moderate DR’, 872 as ’Proliferative DR’, and 708 as ’Severe DR’. The images were captured at different \nresolutions, but for the purposes of this study, they were resized to 224 × 224 pixels to ensure uniform input \ndimensions across the dataset. Preprocessing included the normalization of pixel values to a [0,1] range and data \naugmentation techniques such as random horizontal flipping, zooming, and rotation to introduce variability and \nenhance model generalization.\nData preprocessing\nThe dataset presented a notable imbalance in class distribution, with the dominant class (No DR) \nrepresenting over 70% of the total images. To rectify this disparity, undersampling was employed through the \nRandomUnderSampler function from the imbalanced-learn library. This adjustment reduced the \ncount of ’No DR’ images to match the sample size of the minority class (Severe DR), achieving a balanced dataset \nwith 3704 images. This balancing allowed the model to learn equally from each class, minimizing the likelihood \nof bias toward the majority class during training. The balanced dataset was then divided into 2963 images for \ntraining and 741 images for validation, preserving the balanced class distribution in both subsets.\nAs part of essential preprocessing, data normalization was applied to standardize pixel values across input \nimages. Using Tensor\n---CHUNK_BREAK---\nFlow’s ImageDataGenerator, pixel values were scaled to a range of [0,1] by dividing \neach pixel by 255. To further enhance the training data, data augmentation techniques were incorporated, \nincluding random horizontal flips, rotations up to 0.1 radians, and zooms up to 10%. These augmentations \nintroduced minor variations to the training images, aiding in reducing overfitting and enhancing the model’s \ngeneralization to new data.\nDeep CNN model for comparison\nIn addition to the EfficientNetB0 model, the deep CNN model from Luo et al.25 was referenced for comparative \nperformance assessment. Luo’s model25 architecture leverages a multi-scale feature fusion approach specifically \ndesigned to improve diabetic retinopathy detection by capturing both local and global retinal features, which \nis particularly beneficial for identifying subtle pathologies. This model comprises multiple convolutional layers, \neach followed by pooling operations, and integrates attention mechanisms to focus on essential image areas for \nenhanced diagnostic accuracy. Additionally, Luo et al.25optimized the model by applying multi-scale feature \nfusion layers, allowing effective feature extraction at different spatial resolutions.\nWhile our EfficientNetB0 model uses data augmentation techniques similar to those described in Luo’s \nstudy25, including random rotations, flips, and zooms, it was trained with a larger batch size of 32 over 40 \nepochs. The model was optimized using the Adam optimizer and Sparse Categorical Cross-Entropy as the loss \nfunction. By comparing our EfficientNetB0 model’s performance to Luo et al.25 CNN model, we aim to evaluate \nboth accuracy and computational efficiency in diabetic retinopathy detection. This comparison highlights \nEfficientNetB0’s balance of computational efficiency and robustness, even as Luo’s model25 demonstrates strong \nfeature extraction capabilities through multi-scale processing, enhancing its performance in identifying nuanced \ndiabetic retinopathy features.\nHyperparameters used\nTable 6 below gives a summary for Hyperparameters and their optimization methods for numerous Machine \nLearning algorithms47.\nS. no.\nCategory\nModel used\nParameter\nInput value\n1\nSequential\nEfficientNet B0\ninput_shape\n(224,224,3)\n2\nSequential\nEfficientNet B0\nweights\nimagenet\n3\nSequential\nEfficientNet B0\ninclude_top\nFalse\n4\nLayers\nDense\nNumber of neurons\n5\n5\nLayers\nDense\nactivation\nsoftmax\nTable 6.  Hyperparameters used in model training.\n \nScientific Reports |        (2024) 14:30554 \n11\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 12 ---\nModel evaluation\nThe model’s performance is rigorously assessed to determine its diagnostic precision. Evaluation is conducted \nusing the testing dataset, which was set aside after data partitioning.\nPrecision (Pc) is defined as:\n\t\nPc =\nTruePositivesc\nTruePositivesc + FalsePositivesc \b\n(1)\nRecall (Rc) is defined as:\n\t\nRc =\nTruePositivesc\nTruePositivesc + Fal\n---CHUNK_BREAK---\nseNegativesc\n\b\n(2)\nThe F1 Score calculation for a specific class (c) is defined as:\n\t\nF1c = 2 · Pc · Rc\nPc + Rc \b\n(3)\nExperimental results\nThe EfficientNetB0 model achieved a maximum accuracy of 97.11% with a corresponding loss of 0.1596 after \ntraining for 40 epochs, incorporating data augmentation techniques. Data augmentation, including random \nflips, rotations, and zooms, provided a slight improvement in validation performance, as reflected by an average \naccuracy of 86.53 and an average loss of 0.5663, with 95% confidence intervals for accuracy and loss at (0.8677, \n0.8677) and (0.5529, 0.5529), respectively. During training, accuracy consistently improved, reaching a final \nmaximum training accuracy of 96.87% without data augmentation and 97.11% with augmentation as seen in \nFig. 8. This increase suggests that data augmentation contributes minor but meaningful gains in generalization. \nDespite some variability in validation accuracy, EfficientNetB0 demonstrated competitive performance in \ndiabetic retinopathy classification, indicating its effectiveness in handling this classification task. Table 7 \nsummarizes the results as well as shows the comparison with a previous research.\nAfter 40 epochs, the EfficientNetB0 model achieved an average training accuracy of 86.53% with data \naugmentation applied. In comparison, Luo et al.25 multi-scale feature fusion method, although computationally \nintensive, demonstrated robustness in diabetic retinopathy classification, achieving 83.6% accuracy. Luo’s model25 \neffectively captures local and global retinal features, a valuable capability for nuanced feature identification. \nHowever, EfficientNetB0 maintains a high accuracy while offering significantly lower computational demand \n(processing time of 1190 seconds versus 4778 seconds for Luo’s model)25, making it highly suitable for resource-\nlimited environments.\nBoth the EfficientNetB0 and Luo et al.25 multi-scale feature fusion model were evaluated on diabetic \nretinopathy classification tasks, with each presenting distinct strengths across key performance metrics. The \nEfficientNetB0 model, leveraging pre-trained weights and optimized architecture, achieved an impressive \nmaximum accuracy of 97.11% and demonstrated higher computational efficiency. Luo’s model25, while excelling \nin feature extraction at multiple spatial scales with a precision of 81.9%, incurs a higher processing cost. This \ncomparison highlights that while both models effectively handle retinal images, EfficientNetB0 offers an optimal \nbalance between computational efficiency and accuracy.\nDiscussions and limitations\nOne significant limitation of this study is the potential bias introduced by the dataset. The ’Diagnosis of Diabetic \nRetinopathy’ dataset, though substantial, may lack diversity in patient demographics such as age, gender, and \nethnic background, potentially affecting the model’s generalizability to other populations. This lack of diversity \ncould result in a model tha\n---CHUNK_BREAK---\nt performs well on certain demographic groups but may yield lower accuracy when \napplied to more diverse or distinct populations, limiting its broader clinical applicability.\nAdditionally, while undersampling was used to address class imbalance, this technique may not fully eliminate \nbias, as it reduces the sample size of the dominant classes rather than enriching minority class representation. \nThis could lead to skewed predictions, particularly in underrepresented classes, possibly resulting in lower \ndiagnostic reliability for certain stages or categories of diabetic retinopathy. Future research should consider \nincorporating datasets that represent a wider range of patient characteristics to enhance model robustness across \ndifferent demographics, potentially mitigating these biases.\nFurthermore, while this study used a balanced dataset to assess model performance, its applicability to real-\nworld, naturally imbalanced data remains unexplored. In a real clinical setting, diabetic retinopathy severity \nlevels are unevenly distributed, with milder stages being far more common. A model’s performance on such \nimbalanced data could vary significantly, as it may struggle to detect rare but clinically important cases. Thus, \nexploring model performance on unbalanced datasets is crucial, as it could reveal further tuning needs to ensure \nreliable predictions across all severity levels in practical applications.\nConclusion and future scope\nThis paper proposed a deep learning framework for diagnosing diabetic retinopathy using retinal images. \nThe proposed approach combines the efficiency of EfficientNet with the interpretability of layrering methods \nto achieve accurate and transparent diagnosis. The experimental results demonstrate that EfficientNetB0, \nwith its compound scaling and pre-trained weights, significantly achieves high accuracy and robustness in \nScientific Reports |        (2024) 14:30554 \n12\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 13 ---\nDR classification. The model’s average performance metrics-accuracy of 0.8653 and loss of 0.5663 with data \naugmentation-further support its efficacy, while EfficientNetB0’s 97.11% maximum accuracy, combined with \nlower computational demands, indicates its practical viability for real-world clinical applications, especially \nwhen computational resources are limited. In contrast, Luo et al.25 approach, while effective in feature extraction, \nmay be more suitable for scenarios where detailed feature mapping and high sensitivity are essential, albeit at a \nhigher computational cost.\nFig. 8.  Results achieved: (a) Using EfficientNetB0 without data augmentation; (b) Using EfficientNetB0 with \ndata augmentation.\n \nScientific Reports |        (2024) 14:30554 \n13\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n--- Page 14 ---\nThere are several avenues for future work that can build upon the findings of this stud\n---CHUNK_BREAK---\ny. An immediate \narea for improvement involves expanding the dataset by integrating clinical data, such as patient history, blood \nglucose levels, and the presence of comorbidities like hypertension and cardiovascular diseases. Such additional \ninformation could provide a more comprehensive view of each patient’s health profile48, enabling more precise \npredictions of diabetic retinopathy progression. Moreover, while this study focused on retinal images, future \nresearch could explore multimodal approaches that combine image data with structured medical data to improve \nboth diagnostic accuracy and model robustness.\nIn terms of model development, investigating more advanced architectures such as Vision Transformers \n(ViTs) or hybrid models incorporating attention mechanisms could further boost diagnostic accuracy. Vision \nTransformers have shown great promise in other computer vision tasks and could be adapted for retinal image \nanalysis. Another key direction would be optimizing the model for real-time deployment in clinical settings, \nfocusing on reducing inference time and computational requirements without compromising accuracy. Finally, \nlongitudinal studies that monitor patients over time could yield insights into the model’s ability to predict disease \nprogression, rather than simply classifying the current stage of diabetic retinopathy. Such advancements could \nsignificantly enhance the clinical utility of diabetic retinopathy classification models.\nData availibility\nThe datasets analyzed during the current study are publicly available in the Kaggle repository under the title \n“Diabetic Retinopathy Resized Dataset”46 at ​h​t​t​p​s​:​/​/​w​w​w​.​k​a​g​g​l​e​.​c​o​m​/​d​a​t​a​s​e​t​s​/​t​a​n​l​i​k​e​s​m​a​t​h​/​d​i​a​b​e​t​i​c​-​r​e​t​i​n​o​p​a​t​\nh​y​-​r​e​s​i​z​e​d​/​d​a​t​a​.​\nReceived: 29 September 2024; Accepted: 25 November 2024\nReferences\n\t 1.\t Stitt, A. W. et al. The progress in understanding and treatment of diabetic retinopathy. Prog. Retin. Eye Res. 51, 156–186. ​h​t​t​p​s​:​/​/​d​\no​i​.​o​r​g​/​1​0​.​1​0​1​6​/​j​.​p​r​e​t​e​y​e​r​e​s​.​2​0​1​5​.​0​8​.​0​0​1​ (2016).\n\t 2.\t Gadekallu, T. et al. Deep neural networks to predict diabetic retinopathy. J. Ambient Intell. Hum. Comput. 14, 5407–5420. ​h​t​t​p​s​:​/​/​\nd​o​i​.​o​r​g​/​1​0​.​1​0​0​7​/​s​1​2​6​5​2​-​0​2​0​-​0​1​9​6​3​-​7​ (2023).\n\t 3.\t Silva, P. S. et al. Automated machine learning for predicting diabetic retinopathy progression from ultra-widefield retinal images. \nJAMA Ophthalmol. 142, 171–178. https://doi.org/10.1001/jamaophthalmol.2023.6318 (2024).\n\t 4.\t Grzybowski, A. et al. Artificial intelligence for diabetic retinopathy screening using color retinal photographs: From development \nto deployment. Ophthalmol. Ther. 12, 1419–1437. https://doi.org/10.1007/s40123-023-00691-3 (2023).\n\t 5.\t Vijayan, M. A regression-based approach to diabetic retinopathy diagnosis using efficientnet. Diagnostics 13, 774. ​h​t​t​p​s​:​/​/​d​o​i​.​o​r​g​/​\n1​0​.​3​3​9​0​/​d​i​a​g​n​o​s​t​i​c​s\n---CHUNK_BREAK---\n​1​3​0​4​0​7​7​4​ (2023).\n\t 6.\t Ariza López, L. & Ramos, S. Diabetic retinopathy diagnosis using deep learning (2023).\n\t 7.\t Pramudhita, D. A., Azzahra, F., Arfat, I. K., Magdalena, R. & Saidah, S. Strawberry plant diseases classification using CNN based \non MobileNetV3-large and efficientnet-B0 architecture. Jurnal Ilmiah Teknik Elektro Komputer dan Informatika JITEKI 9, 522–534. \nhttps://doi.org/10.26555/jiteki.v9i3.26341 (2023).\n\t 8.\t Alhijaj, J. A. & Khudeyer, R. S. Integration of efficientnetb0 and machine learning for fingerprint classification. \nInformatica[SPACE]https://doi.org/10.31449/inf.v47i5.4724 (2023).\n\t 9.\t Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus \nphotographs. JAMA 316, 2402–2410 (2016).\n\t10.\t Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using \nretinal images from multiethnic populations with diabetes. JAMA 318, 2211–2223 (2017).\n\t11.\t Abràmoff, M. D., Lavin, P. T., Birch, M., Shah, N. & Folk, J. C. Pivotal trial of an autonomous ai-based diagnostic system for \ndetection of diabetic retinopathy in primary care offices. NPJ Digital Med. 1, 1–8 (2018).\n\t12.\t Li, Z. et al. An automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color \nfundus photographs. Diabetes Care 42, 1584–1589 (2019).\n\t13.\t Bellemo, V. et al. Artificial intelligence using deep learning to screen for referable and vision-threatening diabetic retinopathy in \nAfrica: A clinical validation study. Lancet Digit. Health 1, e35–e44 (2019).\n\t14.\t Bhaskaranand, M. et al. Automated diabetic retinopathy screening and monitoring using retinal fundus image analysis. J. Diabetes \nSci. Technol. 13, 438–446 (2019).\n\t15.\t Sahlsten, J. et al. Development and validation of a deep learning algorithm for diabetic retinopathy and related eye diseases using \nretinal images from multiethnic populations with diabetes: A population-based study. Acta Ophthalmol. 98, e20–e28 (2020).\n\t16.\t Raman, R. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy using non-\nmydriatic retinal fundus images archived in electronic health records. Indian J. Ophthalmol. 68, 398–403 (2020).\n\t17.\t Takahashi, H. et al. Application of deep learning to the determination of diabetic retinopathy and glaucoma based on retinal \nfundus photographs. Jpn. J. Ophthalmol. 64, 368–375 (2020).\n\t18.\t Burlina, P. M. et al. Automated grading of age-related macular degeneration from color fundus images using deep convolutional \nneural networks. JAMA Ophthalmol. 138, 652–659 (2020).\n\t19.\t Huang, K., Zhang, L., Chen, Y., Xie, J. & Li, L. Saliency-guided self-supervised transformer for diabetic retinopathy grading. IEEE \nTrans. Med. Imaging[SPACE]https://doi.org/10.1109/TMI.2024.3045002 (2024).\nModel\nAccuracy\nLoss\nEfficientNetB0\nWithout data augmentation\n0.8622\n\n---CHUNK_BREAK---\n0.5783\nWith data augmentation\n0.8653\n0.5663\nDeep CNN Model\n–\n0.836\n–\nTable 7.  Average accuracy with corresponding loss.\n \nScientific Reports |        (2024) 14:30554 \n14\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 15 ---\n\t20.\t Thanikachalam, R., Sivakumar, M., Kalyani, M. & Kumar, H. Optimized deep convolutional neural networks for diabetic \nretinopathy and macular edema detection. Comput. Biol. Med.[SPACE]https://doi.org/10.1016/j.compbiomed.2024.105034 \n(2024).\n\t21.\t Bodapati, S. & Balaji, K. Self-adaptive stacking ensemble with attention mechanisms for diabetic retinopathy severity prediction. \nArtif. Intell. Med.[SPACE]https://doi.org/10.1016/j.artmed.2024.101850 (2024).\n\t22.\t Bhati, S., Singh, P. & Thakur, R. Idanet: Interpretable dual attention network for diabetic retinopathy grading. IEEE \nAccess[SPACE]https://doi.org/10.1109/ACCESS.2024.3100089 (2024).\n\t23.\t Sivapriya, R., Chithra, M. & Ragavendran, C. Microvascular structure analysis in diabetic retinopathy classification using deep \nlearning. IEEE J. Biomed. Health Inform.[SPACE]https://doi.org/10.1109/JBHI.2024.3120031 (2024).\n\t24.\t Ohri, P. & Kumar, R. Supervised fine-tuned approach for diabetic retinopathy detection using transfer learning. J. Digit. \nImaging[SPACE]https://doi.org/10.1007/s10278-024-00609-1 (2024).\n\t25.\t Luo, Z., Wang, F. & Zheng, Q. Deep CNN model with multi-scale feature fusion for diabetic retinopathy detection. Expert Syst. \nAppl.[SPACE]https://doi.org/10.1016/j.eswa.2024.117845 (2024).\n\t26.\t Romero-Oraá, M. T., Martinez, J., Gonzalez, R. & Gonzalez, M. Attention-based framework for diabetic retinopathy grading in \nfundus images. Biomed. Signal Process. Control[SPACE]https://doi.org/10.1016/j.bspc.2024.105564 (2024).\n\t27.\t Zhang, Y., Wang, Y., Wu, X. & Li, J. Semi-supervised contrastive learning with saliency maps for diabetic retinopathy classification. \nPattern Recogn.[SPACE]https://doi.org/10.1016/j.patcog.2024.109164 (2024).\n\t28.\t Wong, L., Yang, H. & Zheng, Y. Transfer learning with feature-weighted ECOC ensembles for diabetic retinopathy grading. \nComput. Biol. Med.[SPACE]https://doi.org/10.1016/j.compbiomed.2023.105489 (2023).\n\t29.\t Bilal, A., Zhu, L., Deng, A., Lu, H. & Wu, N. Ai-based automatic detection and classification of diabetic retinopathy using u-net \nand deep learning. Symmetry 14, 1427. https://doi.org/10.3390/sym14071427 (2022).\n\t30.\t Bilal, A., Sun, G., Li, Y., Mazhar, S. & Khan, A. Diabetic retinopathy detection and classification using mixed models for a disease \ngrading database. IEEE Access[SPACE]https://doi.org/10.1109/ACCESS.2021.3056186 (2021).\n\t31.\t Bilal, A., Mazhar, S., Imran, A. & Latif, J. A transfer learning and u-net-based automatic detection of diabetic retinopathy from \nfundus images. Comput. Methods Biomech. Biomed. Eng. Imaging ​V​i​s​u​a​l​i​z​.​[​S​P​A​C​E​]​h​t​t​p​s​:​/​/​d​o​i​.​o​r​g​/​1​0​.​1​0​8​0​/​2​1​6​8​1​1​6​3​.​2​0​2​1​.​2​0​2​1​1​1​\n1​ (2022).\n\t\n---CHUNK_BREAK---\n32.\t Bilal, A., Sun, G., Mazhar, S. & Imran, A. Improved grey wolf optimization-based feature selection and classification using CNN \nfor diabetic retinopathy detection. 1–14 (2022).\n\t33.\t Bilal, A., Sun, G. & Mazhar, S. Diabetic retinopathy detection using weighted filters and classification using CNN. ​h​t​t​p​s​:​/​/​d​o​i​.​o​r​g​/​\n1​0​.​1​1​0​9​/​C​O​N​I​T​5​1​4​8​0​.​2​0​2​1​.​9​4​9​8​4​6​6​ (2021).\n\t34.\t Bilal, A. et al. Improved support vector machine based on CNN-SVD for vision-threatening diabetic retinopathy detection and \nclassification. PLoS One 19, e0295951. https://doi.org/10.1371/journal.pone.0295951 (2024).\n\t35.\t Bilal, A., Liu, X., Baig, T., Long, H. & Shafiq, M. Edgesvdnet: 5g-enabled detection and classification of vision-threatening diabetic \nretinopathy in retinal fundus images. Electronics 12, 4094. https://doi.org/10.3390/electronics12194094 (2023).\n\t36.\t Bilal, A., Liu, X., Shafiq, M., Ahmed, Z. & Long, H. Nimeq-sacnet: A novel self-attention precision medicine model for vision-\nthreatening diabetic retinopathy using image data. Comput. Biol. Med. 171, 108099. ​h​t​t​p​s​:​/​/​d​o​i​.​o​r​g​/​1​0​.​1​0​1​6​/​j​.​c​o​m​p​b​i​o​m​e​d​.​2​0​2​4​.​1​\n0​8​0​9​9​ (2024).\n\t37.\t Gupta, B. B., Gaurav, A. & Panigrahi, P. K. Analysis of security and privacy issues of information management of big data in B2B \nbased healthcare systems. J. Bus. Res. 162, 113859 (2023).\n\t38.\t Zaidan, A. A., AlSattar, H. A., Qahtan, S., Deveci, M. & Pamucar, D. Secure decision approach for internet of healthcare things \nsmart systems-based blockchain. IEEE Internet of Things Journal (2023).\n\t39.\t Zhou, Y., Song, L., Liu, Y. & Vijayakumar, P. A privacy-preserving logistic regression-based diagnosis scheme for digital healthcare. \nFutur. Gener. Comput. Syst. 144, 63–73 (2023).\n\t40.\t Singh, S. K. Linux yourself: Concept and programming 1st edn. (Chapman and Hall/CRC, Cham, 2021).\n\t41.\t Chui, K. T. et al. Multiround transfer learning and modified generative adversarial network for lung cancer detection. Int. J. Intell. \nSyst. 2023, 6376275 (2023).\n\t42.\t Hammad, M., Abd El-Latif, A. A., Hussain, A. & Abd El-Samie, F. E. Deep learning models for arrhythmia detection in IoT \nhealthcare applications. Comput. Electr. Eng. 100, 108011 (2022).\n\t43.\t Sutomo, H. I. Identification of organic and non-organic waste with computer image recognition using convolutionalneural \nnetwork with efficient-net-b0 architecture. J. Appl. Intell. Syst. 8(3), 320–330. https://doi.org/10.33633/jais.v8i3.9064 (2023).\n\t44.\t Y, V., Billakanti, N., Veeravalli, K., N, A. D. R. & Kota, L. Early detection of casava plant leaf diseases using efficientnet-b0. In 2022 \nIEEE Delhi Section Conference (DELCON), 1–5, https://doi.org/10.1109/DELCON54057.2022.9753210 (2022).\n\t45.\t Agarwal, V. Complete architectural details of all efficientnet models. Medium (2021). ​h​t​t​p​s​:​/​/​t​o​w​a​r​d​s​d​a​t​a​s​c​i​e​n​c​e​.​c​o​m​/​c​o​m​p​l​e​t​e​-​a​r​\nc​h​i​t​e​c​t​u​r​a​l​-​d​e​t​a​i​l​s​-​o​f​-​a​l​\n---CHUNK_BREAK---\nl​-​e​f​f​i​c​i​e​n​t​n​e​t​-​m​o​d​e​l​s​-​5​f​d​5​b​7​3​6​1​4​2​.​\n\t46.\t TanLikesMath. Diabetic retinopathy - resized data. Kaggle, https://www.​kaggle.com/d​atasets/tanl​ikesmath/di​abetic R-retinopathy, \nhttps:​//d​oi.​or​g/10.​13​1​4​0/RG.​2.2.1​3037.​19​688-resized/data (20243).\n\t47.\t Yang, L. & Shami, A. On hyperparameter optimization of machine learning algorithms: Theory and practice. Neurocomputing 415, \n295–316. https://doi.org/10.1016/j.neucom.2020.07.061 (2020).\n\t48.\t Rathee, G., Saini, H., Garg, S., Choi, B. J. & Hassan, M. M. A secure data e-governance for healthcare application in cyber physical \nsystems. Int. J. Semantic Web Inf. Syst. 20, 1–17 (2024).\nAcknowledgement\nThis research work was funded by Institutional Fund Projects under grant no. (IFPIP-639-611-1443). Therefore, \nthe authors gratefully acknowledge technical and financial support from Ministry of Education and Deanship of \nScientific Research (DSR), King Abdulaziz University (KAU), Jeddah, Saudi Arabia.\nAuthor contributions\nL.A.: Conceptualization, methodology, writing original draft. S.K.S.: Formal analysis, supervision, reviewing \nand editing, data interpretation, supervision. S.K.: Conceptualization, methodology, Data curation, validation, \nwriting original draft, supervision. H.G.: Conceptualization, methodology, writing original draft, Software, visu­\nalization, data interpretation. W.A.: Investigation, resources, manuscript preparation. V.A.: Methodology, valida­\ntion, writing – review and editing. S.B.: Data acquisition, software, writing – review and editing. K.T.C.: Formal \nanalysis, data interpretation, supervision. B.B.G.: Supervision, resources, project oversight, funding acquisition.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nScientific Reports |        (2024) 14:30554 \n15\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n\n---CHUNK_BREAK---\n\n--- Page 16 ---\nAdditional information\nCorrespondence and requests for materials should be addressed to S.K.S. or B.B.G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access   This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not include\n---CHUNK_BREAK---\nd in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit ​h​t​t​p​:​/​/​c​r​e​a​t​i​v​e​c​o​m​m​o​\nn​s​.​o​r​g​/​l​i​c​e​n​s​e​s​/​b​y​-​n​c​-​n​d​/​4​.​0​/​.​\n© The Author(s) 2024 \nScientific Reports |        (2024) 14:30554 \n16\n| https://doi.org/10.1038/s41598-024-81132-4\nwww.nature.com/scientificreports/\n"
}